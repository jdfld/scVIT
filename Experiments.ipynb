{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":47088,"status":"ok","timestamp":1733711170722,"user":{"displayName":"Johan Lindqvist","userId":"10341636458737247069"},"user_tz":480},"id":"-7G0k2ILvreD","outputId":"4e8f497d-e31f-4c6c-ace1-ddd6a0c88193"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting scanpy\n","  Downloading scanpy-1.10.4-py3-none-any.whl.metadata (9.3 kB)\n","Collecting anndata>=0.8 (from scanpy)\n","  Downloading anndata-0.11.1-py3-none-any.whl.metadata (8.2 kB)\n","Requirement already satisfied: h5py>=3.6 in /usr/local/lib/python3.10/dist-packages (from scanpy) (3.12.1)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from scanpy) (1.4.2)\n","Collecting legacy-api-wrap>=1.4 (from scanpy)\n","  Downloading legacy_api_wrap-1.4.1-py3-none-any.whl.metadata (2.1 kB)\n","Requirement already satisfied: matplotlib>=3.6 in /usr/local/lib/python3.10/dist-packages (from scanpy) (3.8.0)\n","Requirement already satisfied: natsort in /usr/local/lib/python3.10/dist-packages (from scanpy) (8.4.0)\n","Requirement already satisfied: networkx>=2.7 in /usr/local/lib/python3.10/dist-packages (from scanpy) (3.4.2)\n","Requirement already satisfied: numba>=0.56 in /usr/local/lib/python3.10/dist-packages (from scanpy) (0.60.0)\n","Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.10/dist-packages (from scanpy) (1.26.4)\n","Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from scanpy) (24.2)\n","Requirement already satisfied: pandas>=1.5 in /usr/local/lib/python3.10/dist-packages (from scanpy) (2.2.2)\n","Requirement already satisfied: patsy!=1.0.0 in /usr/local/lib/python3.10/dist-packages (from scanpy) (1.0.1)\n","Collecting pynndescent>=0.5 (from scanpy)\n","  Downloading pynndescent-0.5.13-py3-none-any.whl.metadata (6.8 kB)\n","Requirement already satisfied: scikit-learn>=1.1 in /usr/local/lib/python3.10/dist-packages (from scanpy) (1.5.2)\n","Requirement already satisfied: scipy>=1.8 in /usr/local/lib/python3.10/dist-packages (from scanpy) (1.13.1)\n","Requirement already satisfied: seaborn>=0.13 in /usr/local/lib/python3.10/dist-packages (from scanpy) (0.13.2)\n","Collecting session-info (from scanpy)\n","  Downloading session_info-1.0.0.tar.gz (24 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: statsmodels>=0.13 in /usr/local/lib/python3.10/dist-packages (from scanpy) (0.14.4)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from scanpy) (4.66.6)\n","Collecting umap-learn!=0.5.0,>=0.5 (from scanpy)\n","  Downloading umap_learn-0.5.7-py3-none-any.whl.metadata (21 kB)\n","Collecting array-api-compat!=1.5,>1.4 (from anndata>=0.8->scanpy)\n","  Downloading array_api_compat-1.9.1-py3-none-any.whl.metadata (1.6 kB)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anndata>=0.8->scanpy) (1.2.2)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->scanpy) (1.3.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->scanpy) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->scanpy) (4.55.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->scanpy) (1.4.7)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->scanpy) (11.0.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->scanpy) (3.2.0)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->scanpy) (2.8.2)\n","Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.56->scanpy) (0.43.0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.5->scanpy) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.5->scanpy) (2024.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.1->scanpy) (3.5.0)\n","Collecting stdlib_list (from session-info->scanpy)\n","  Downloading stdlib_list-0.11.0-py3-none-any.whl.metadata (3.3 kB)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.6->scanpy) (1.16.0)\n","Downloading scanpy-1.10.4-py3-none-any.whl (2.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading anndata-0.11.1-py3-none-any.whl (141 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading legacy_api_wrap-1.4.1-py3-none-any.whl (10.0 kB)\n","Downloading pynndescent-0.5.13-py3-none-any.whl (56 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.9/56.9 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading umap_learn-0.5.7-py3-none-any.whl (88 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.8/88.8 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading array_api_compat-1.9.1-py3-none-any.whl (50 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m989.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading stdlib_list-0.11.0-py3-none-any.whl (83 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.6/83.6 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: session-info\n","  Building wheel for session-info (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for session-info: filename=session_info-1.0.0-py3-none-any.whl size=8023 sha256=c47a9e80bc62f104a62fd7648cca8f2e4d65f19779ac306a49929e82d854feb1\n","  Stored in directory: /root/.cache/pip/wheels/6a/aa/b9/eb5d4031476ec10802795b97ccf937b9bd998d68a9b268765a\n","Successfully built session-info\n","Installing collected packages: stdlib_list, legacy-api-wrap, array-api-compat, session-info, pynndescent, anndata, umap-learn, scanpy\n","Successfully installed anndata-0.11.1 array-api-compat-1.9.1 legacy-api-wrap-1.4.1 pynndescent-0.5.13 scanpy-1.10.4 session-info-1.0.0 stdlib_list-0.11.0 umap-learn-0.5.7\n","Collecting scikit-misc\n","  Downloading scikit_misc-0.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.0 kB)\n","Requirement already satisfied: numpy>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from scikit-misc) (1.26.4)\n","Downloading scikit_misc-0.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (188 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.8/188.8 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: scikit-misc\n","Successfully installed scikit-misc-0.5.1\n","Requirement already satisfied: anndata in /usr/local/lib/python3.10/dist-packages (0.11.1)\n","Requirement already satisfied: array-api-compat!=1.5,>1.4 in /usr/local/lib/python3.10/dist-packages (from anndata) (1.9.1)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anndata) (1.2.2)\n","Requirement already satisfied: h5py>=3.6 in /usr/local/lib/python3.10/dist-packages (from anndata) (3.12.1)\n","Requirement already satisfied: natsort in /usr/local/lib/python3.10/dist-packages (from anndata) (8.4.0)\n","Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.10/dist-packages (from anndata) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from anndata) (24.2)\n","Requirement already satisfied: pandas!=2.1.0rc0,!=2.1.2,>=1.4 in /usr/local/lib/python3.10/dist-packages (from anndata) (2.2.2)\n","Requirement already satisfied: scipy>1.8 in /usr/local/lib/python3.10/dist-packages (from anndata) (1.13.1)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas!=2.1.0rc0,!=2.1.2,>=1.4->anndata) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas!=2.1.0rc0,!=2.1.2,>=1.4->anndata) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas!=2.1.0rc0,!=2.1.2,>=1.4->anndata) (2024.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas!=2.1.0rc0,!=2.1.2,>=1.4->anndata) (1.16.0)\n","Collecting pyro-ppl\n","  Downloading pyro_ppl-1.9.1-py3-none-any.whl.metadata (7.8 kB)\n","Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.10/dist-packages (from pyro-ppl) (1.26.4)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from pyro-ppl) (3.4.0)\n","Collecting pyro-api>=0.1.1 (from pyro-ppl)\n","  Downloading pyro_api-0.1.2-py3-none-any.whl.metadata (2.5 kB)\n","Requirement already satisfied: torch>=2.0 in /usr/local/lib/python3.10/dist-packages (from pyro-ppl) (2.5.1+cu121)\n","Requirement already satisfied: tqdm>=4.36 in /usr/local/lib/python3.10/dist-packages (from pyro-ppl) (4.66.6)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->pyro-ppl) (3.16.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->pyro-ppl) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->pyro-ppl) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->pyro-ppl) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->pyro-ppl) (2024.10.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->pyro-ppl) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=2.0->pyro-ppl) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.0->pyro-ppl) (3.0.2)\n","Downloading pyro_ppl-1.9.1-py3-none-any.whl (755 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m756.0/756.0 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pyro_api-0.1.2-py3-none-any.whl (11 kB)\n","Installing collected packages: pyro-api, pyro-ppl\n","Successfully installed pyro-api-0.1.2 pyro-ppl-1.9.1\n","Collecting pytorch-forecasting\n","  Downloading pytorch_forecasting-1.2.0-py3-none-any.whl.metadata (13 kB)\n","Requirement already satisfied: numpy<=3.0.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-forecasting) (1.26.4)\n","Requirement already satisfied: torch!=2.0.1,<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-forecasting) (2.5.1+cu121)\n","Collecting lightning<3.0.0,>=2.0.0 (from pytorch-forecasting)\n","  Downloading lightning-2.4.0-py3-none-any.whl.metadata (38 kB)\n","Requirement already satisfied: scipy<2.0,>=1.8 in /usr/local/lib/python3.10/dist-packages (from pytorch-forecasting) (1.13.1)\n","Requirement already satisfied: pandas<3.0.0,>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-forecasting) (2.2.2)\n","Requirement already satisfied: scikit-learn<2.0,>=1.2 in /usr/local/lib/python3.10/dist-packages (from pytorch-forecasting) (1.5.2)\n","Requirement already satisfied: PyYAML<8.0,>=5.4 in /usr/local/lib/python3.10/dist-packages (from lightning<3.0.0,>=2.0.0->pytorch-forecasting) (6.0.2)\n","Requirement already satisfied: fsspec<2026.0,>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning<3.0.0,>=2.0.0->pytorch-forecasting) (2024.10.0)\n","Collecting lightning-utilities<2.0,>=0.10.0 (from lightning<3.0.0,>=2.0.0->pytorch-forecasting)\n","  Downloading lightning_utilities-0.11.9-py3-none-any.whl.metadata (5.2 kB)\n","Requirement already satisfied: packaging<25.0,>=20.0 in /usr/local/lib/python3.10/dist-packages (from lightning<3.0.0,>=2.0.0->pytorch-forecasting) (24.2)\n","Collecting torchmetrics<3.0,>=0.7.0 (from lightning<3.0.0,>=2.0.0->pytorch-forecasting)\n","  Downloading torchmetrics-1.6.0-py3-none-any.whl.metadata (20 kB)\n","Requirement already satisfied: tqdm<6.0,>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from lightning<3.0.0,>=2.0.0->pytorch-forecasting) (4.66.6)\n","Requirement already satisfied: typing-extensions<6.0,>=4.4.0 in /usr/local/lib/python3.10/dist-packages (from lightning<3.0.0,>=2.0.0->pytorch-forecasting) (4.12.2)\n","Collecting pytorch-lightning (from lightning<3.0.0,>=2.0.0->pytorch-forecasting)\n","  Downloading pytorch_lightning-2.4.0-py3-none-any.whl.metadata (21 kB)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0.0,>=1.3.0->pytorch-forecasting) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0.0,>=1.3.0->pytorch-forecasting) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0.0,>=1.3.0->pytorch-forecasting) (2024.2)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<2.0,>=1.2->pytorch-forecasting) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<2.0,>=1.2->pytorch-forecasting) (3.5.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch!=2.0.1,<3.0.0,>=2.0.0->pytorch-forecasting) (3.16.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch!=2.0.1,<3.0.0,>=2.0.0->pytorch-forecasting) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch!=2.0.1,<3.0.0,>=2.0.0->pytorch-forecasting) (3.1.4)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch!=2.0.1,<3.0.0,>=2.0.0->pytorch-forecasting) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch!=2.0.1,<3.0.0,>=2.0.0->pytorch-forecasting) (1.3.0)\n","Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning<3.0.0,>=2.0.0->pytorch-forecasting) (3.11.9)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities<2.0,>=0.10.0->lightning<3.0.0,>=2.0.0->pytorch-forecasting) (75.1.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3.0.0,>=1.3.0->pytorch-forecasting) (1.16.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch!=2.0.1,<3.0.0,>=2.0.0->pytorch-forecasting) (3.0.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning<3.0.0,>=2.0.0->pytorch-forecasting) (2.4.4)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning<3.0.0,>=2.0.0->pytorch-forecasting) (1.3.1)\n","Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning<3.0.0,>=2.0.0->pytorch-forecasting) (4.0.3)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning<3.0.0,>=2.0.0->pytorch-forecasting) (24.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning<3.0.0,>=2.0.0->pytorch-forecasting) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning<3.0.0,>=2.0.0->pytorch-forecasting) (6.1.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning<3.0.0,>=2.0.0->pytorch-forecasting) (0.2.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning<3.0.0,>=2.0.0->pytorch-forecasting) (1.18.3)\n","Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning<3.0.0,>=2.0.0->pytorch-forecasting) (3.10)\n","Downloading pytorch_forecasting-1.2.0-py3-none-any.whl (181 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.9/181.9 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading lightning-2.4.0-py3-none-any.whl (810 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m811.0/811.0 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading lightning_utilities-0.11.9-py3-none-any.whl (28 kB)\n","Downloading torchmetrics-1.6.0-py3-none-any.whl (926 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m926.4/926.4 kB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pytorch_lightning-2.4.0-py3-none-any.whl (815 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m815.2/815.2 kB\u001b[0m \u001b[31m35.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: lightning-utilities, torchmetrics, pytorch-lightning, lightning, pytorch-forecasting\n","Successfully installed lightning-2.4.0 lightning-utilities-0.11.9 pytorch-forecasting-1.2.0 pytorch-lightning-2.4.0 torchmetrics-1.6.0\n"]}],"source":["!pip install scanpy\n","!pip install scikit-misc\n","!pip install anndata\n","!pip install pyro-ppl\n","!pip install pytorch-forecasting\n","!pip install scvi-tools"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16526,"status":"ok","timestamp":1733711187678,"user":{"displayName":"Johan Lindqvist","userId":"10341636458737247069"},"user_tz":480},"id":"zmMrztXbwHz6","outputId":"d4277df5-5f6c-43d8-a213-63ef43a35217"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["# Load the Drive helper and mount\n","from google.colab import drive\n","drive.mount('/content/drive')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SvFVY6Ogt02X"},"outputs":[],"source":["#import drive.MyDrive.CSE527.GeneDataset as gd, custom dataloader is not necessary for these tasks and slightly slower than annloader\n","import anndata\n","import scanpy as sc\n","import torch\n","import model as m\n","import network_utils as n\n","import scvi\n","import numpy as np\n","from anndata.experimental import AnnLoader\n","from importlib import reload\n","from sklearn.preprocessing import LabelEncoder"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GJq8rjUrysc_"},"outputs":[],"source":["adata = sc.read_h5ad('drive/MyDrive/CSE527/data_chunk2.h5ad')\n","adata = adata[:,adata.var.sort_values(by=\"Gene\").index]\n","aloader = anndata.experimental.AnnLoader(adata,batch_size=1024,shuffle=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kc7DML4mzHTe"},"outputs":[],"source":["enc_lin = [512,256,64]\n","dec_lin = [512,256,64]\n","soi = [[40,500,1487,4],[40,100,500,4],[40,20,100,4],[20,5,40,1]]\n","auto_transformer = m.TransformerAutoencoder(input_size=59480, soih=soi,enc_lin = enc_lin, dec_lin = dec_lin, lr = 0.005, loss=\"ZINB\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9117,"status":"ok","timestamp":1733711359482,"user":{"displayName":"Johan Lindqvist","userId":"10341636458737247069"},"user_tz":480},"id":"Gmb8ZASORrzi","outputId":"98842d78-e0b0-4cad-b030-075aee57c591"},"outputs":[{"data":{"text/plain":["<All keys matched successfully>"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["PATH = \"trained_auto_transformerZINB9\"\n","auto_transformer.load_state_dict(torch.load(PATH, weights_only=True))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H1ycpZZT9mnt"},"outputs":[],"source":["auto_transformer = auto_transformer.to('cuda')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ghy-jM9g0H8K"},"outputs":[],"source":["bdata = sc.read_h5ad('gridchunk1_1_2.h5ad')\n","bdata = bdata[:,bdata.var.sort_values(by=\"Gene\").index]\n","bloader = anndata.experimental.AnnLoader(bdata,batch_size=256,shuffle=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2807253,"status":"ok","timestamp":1733094532394,"user":{"displayName":"Johan Lindqvist","userId":"10341636458737247069"},"user_tz":480},"id":"ejldmaSo1LrD","outputId":"5a431d62-fd9f-42aa-f32d-5414ec938d3a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Batch  1  Loss  3.126366376876831\n","Batch  2  Loss  2.2696211338043213\n","Batch  3  Loss  1.9834163188934326\n","Batch  4  Loss  1.9504767656326294\n","Batch  5  Loss  1.8178988695144653\n","Batch  6  Loss  1.9236786365509033\n","Batch  7  Loss  1.7570282220840454\n","Batch  8  Loss  1.8179502487182617\n","Batch  9  Loss  1.8100172281265259\n","Batch  10  Loss  1.7045681476593018\n","Batch  11  Loss  1.6461560726165771\n","Batch  12  Loss  1.6430808305740356\n","Batch  13  Loss  1.5820934772491455\n","Batch  14  Loss  1.4656087160110474\n","Batch  15  Loss  1.4909175634384155\n","Batch  16  Loss  1.4435691833496094\n","Batch  17  Loss  1.3917804956436157\n","Batch  18  Loss  1.359212040901184\n","Batch  19  Loss  1.3349696397781372\n","Batch  20  Loss  1.2690670490264893\n","Batch  21  Loss  1.338834285736084\n","Batch  22  Loss  1.217712640762329\n","Batch  23  Loss  1.2166188955307007\n","Batch  24  Loss  1.203122615814209\n","Batch  25  Loss  1.1267715692520142\n","Batch  26  Loss  1.1636011600494385\n","Batch  27  Loss  1.1095136404037476\n","Batch  28  Loss  1.0488907098770142\n","Batch  29  Loss  1.022862195968628\n","Batch  30  Loss  0.9788355827331543\n","Batch  31  Loss  0.9596342444419861\n","Batch  32  Loss  0.9188762307167053\n","Batch  33  Loss  0.8270930051803589\n","Batch  34  Loss  0.8318403959274292\n","Batch  35  Loss  0.7930464744567871\n","Batch  36  Loss  0.8001788258552551\n","Batch  37  Loss  0.7545800805091858\n","Batch  38  Loss  0.7233304381370544\n","Batch  39  Loss  0.7262246012687683\n","Batch  40  Loss  0.6876397132873535\n","Batch  41  Loss  0.6505569815635681\n","Batch  42  Loss  0.6511256098747253\n","Batch  43  Loss  0.6152533888816833\n","Batch  44  Loss  0.5843855738639832\n","Batch  45  Loss  0.592889130115509\n","Batch  46  Loss  0.5594164133071899\n","Batch  47  Loss  0.5511458516120911\n","Batch  48  Loss  0.543624997138977\n","Batch  49  Loss  0.5330902338027954\n","Batch  50  Loss  0.5301762819290161\n","Batch  51  Loss  0.5069053769111633\n","Batch  52  Loss  0.49853697419166565\n","Batch  53  Loss  0.5446100831031799\n","Batch  54  Loss  0.5099761486053467\n","Batch  55  Loss  0.4776270389556885\n","Batch  56  Loss  0.47689497470855713\n","Batch  57  Loss  0.45012611150741577\n","Batch  58  Loss  0.46976178884506226\n","Batch  59  Loss  0.4523298740386963\n","Batch  60  Loss  0.4632139801979065\n","Batch  61  Loss  0.4522643983364105\n","Batch  62  Loss  0.457829087972641\n","Batch  63  Loss  0.4276953637599945\n","Batch  64  Loss  0.4619191884994507\n","Batch  65  Loss  0.4357619285583496\n","Batch  66  Loss  0.4423578977584839\n","Batch  67  Loss  0.44256845116615295\n","Batch  68  Loss  0.451305091381073\n","Batch  69  Loss  0.45243895053863525\n","Batch  70  Loss  0.433650404214859\n","Batch  71  Loss  0.4340243339538574\n","Batch  72  Loss  0.4295235574245453\n","Batch  73  Loss  0.43088603019714355\n","Batch  74  Loss  0.4271854758262634\n","Batch  75  Loss  0.42651495337486267\n","Batch  76  Loss  0.442207008600235\n","Batch  77  Loss  0.4268900752067566\n","Batch  78  Loss  0.42292293906211853\n","Batch  79  Loss  0.4076194763183594\n","Batch  80  Loss  0.4265786111354828\n","Batch  81  Loss  0.41426074504852295\n","Batch  82  Loss  0.4225645363330841\n","Batch  83  Loss  0.4174148142337799\n","Batch  84  Loss  0.42402446269989014\n","Batch  85  Loss  0.4159379303455353\n","Batch  86  Loss  0.41256749629974365\n","Batch  87  Loss  0.41260209679603577\n","Batch  88  Loss  0.41278576850891113\n","Batch  89  Loss  0.40202096104621887\n","Batch  90  Loss  0.4123011827468872\n","Batch  91  Loss  0.41778984665870667\n","Batch  92  Loss  0.40897348523139954\n","Batch  93  Loss  0.41528207063674927\n","Batch  94  Loss  0.4203570783138275\n","Batch  95  Loss  0.41774460673332214\n","Batch  96  Loss  0.407851904630661\n","Batch  97  Loss  0.40832334756851196\n","Batch  98  Loss  0.39960727095603943\n","Batch  99  Loss  0.4144108295440674\n","Batch  100  Loss  0.4090222418308258\n","Batch  101  Loss  0.39893949031829834\n","Batch  102  Loss  0.400154709815979\n","Batch  103  Loss  0.40925729274749756\n","Batch  104  Loss  0.3960002064704895\n","Batch  105  Loss  0.3987013101577759\n","Batch  106  Loss  0.391175240278244\n","Batch  107  Loss  0.40714550018310547\n","Batch  108  Loss  0.4207341969013214\n","Batch  109  Loss  0.4130219519138336\n","Batch  110  Loss  0.4076773226261139\n","Batch  111  Loss  0.4167596995830536\n","Batch  112  Loss  0.40029817819595337\n","Batch  113  Loss  0.3941444158554077\n","Batch  114  Loss  0.4125193655490875\n","Batch  115  Loss  0.4156002402305603\n","Batch  116  Loss  0.409107506275177\n","Batch  117  Loss  0.4015503227710724\n","Batch  118  Loss  0.40483012795448303\n","Batch  119  Loss  0.40785419940948486\n","Batch  120  Loss  0.39350396394729614\n","Batch  121  Loss  0.39589473605155945\n","Batch  122  Loss  0.3993382751941681\n","Batch  123  Loss  0.40359577536582947\n","Batch  124  Loss  0.4050208330154419\n","Batch  125  Loss  0.4061296880245209\n","Batch  126  Loss  0.4115806221961975\n","Batch  127  Loss  0.40846961736679077\n","Batch  128  Loss  0.39194226264953613\n","Batch  129  Loss  0.40665945410728455\n","Batch  130  Loss  0.3979273736476898\n","Batch  131  Loss  0.3959174156188965\n","Batch  132  Loss  0.40636810660362244\n","Batch  133  Loss  0.3988533318042755\n","Batch  134  Loss  0.40177419781684875\n","Batch  135  Loss  0.3899729251861572\n","Batch  136  Loss  0.3831958472728729\n","Batch  137  Loss  0.402203768491745\n","Batch  138  Loss  0.4063429832458496\n","Batch  139  Loss  0.40072810649871826\n","Batch  140  Loss  0.3952113687992096\n","Batch  141  Loss  0.38809412717819214\n","Batch  142  Loss  0.39327386021614075\n","Batch  143  Loss  0.40015801787376404\n","Batch  144  Loss  0.3867073953151703\n","Batch  145  Loss  0.39116370677948\n","Batch  146  Loss  0.39287400245666504\n","Batch  147  Loss  0.3834819495677948\n","Batch  148  Loss  0.40011292695999146\n","Batch  149  Loss  0.39536798000335693\n","Batch  150  Loss  0.39701783657073975\n","Batch  151  Loss  0.39706313610076904\n","Batch  152  Loss  0.3908310830593109\n","Batch  153  Loss  0.39948365092277527\n","Batch  154  Loss  0.40225329995155334\n","Batch  155  Loss  0.39204150438308716\n","Batch  156  Loss  0.39949244260787964\n","Batch  157  Loss  0.39117687940597534\n","Batch  158  Loss  0.39549413323402405\n","Batch  159  Loss  0.410050630569458\n","Batch  160  Loss  0.3901066184043884\n","Batch  161  Loss  0.41136443614959717\n","Batch  162  Loss  0.4026431739330292\n","Batch  163  Loss  0.4103085994720459\n","Batch  164  Loss  0.36956050992012024\n","Batch  165  Loss  0.3927166759967804\n","Epoch: 0 , Loss: tensor(0.1057, device='cuda:0', grad_fn=<AddBackward0>)\n","Batch  1  Loss  0.40614941716194153\n","Batch  2  Loss  0.3878003656864166\n","Batch  3  Loss  0.40250495076179504\n","Batch  4  Loss  0.40774181485176086\n","Batch  5  Loss  0.40202462673187256\n","Batch  6  Loss  0.37986302375793457\n","Batch  7  Loss  0.38009244203567505\n","Batch  8  Loss  0.3912597894668579\n","Batch  9  Loss  0.38598817586898804\n","Batch  10  Loss  0.38061439990997314\n","Batch  11  Loss  0.3919951915740967\n","Batch  12  Loss  0.40165582299232483\n","Batch  13  Loss  0.40348580479621887\n","Batch  14  Loss  0.3892248570919037\n","Batch  15  Loss  0.4098570644855499\n","Batch  16  Loss  0.39752325415611267\n","Batch  17  Loss  0.3903992772102356\n","Batch  18  Loss  0.3772476017475128\n","Batch  19  Loss  0.3828815221786499\n","Batch  20  Loss  0.37673085927963257\n","Batch  21  Loss  0.3966020345687866\n","Batch  22  Loss  0.4002549648284912\n","Batch  23  Loss  0.3823784589767456\n","Batch  24  Loss  0.3998841941356659\n","Batch  25  Loss  0.3967847228050232\n","Batch  26  Loss  0.38279417157173157\n","Batch  27  Loss  0.38177675008773804\n","Batch  28  Loss  0.393888920545578\n","Batch  29  Loss  0.40139058232307434\n","Batch  30  Loss  0.39716652035713196\n","Batch  31  Loss  0.37796854972839355\n","Batch  32  Loss  0.38173821568489075\n","Batch  33  Loss  0.38524991273880005\n","Batch  34  Loss  0.378770649433136\n","Batch  35  Loss  0.3766452968120575\n","Batch  36  Loss  0.379815012216568\n","Batch  37  Loss  0.39487794041633606\n","Batch  38  Loss  0.38792258501052856\n","Batch  39  Loss  0.3705744445323944\n","Batch  40  Loss  0.39331457018852234\n","Batch  41  Loss  0.3896951973438263\n","Batch  42  Loss  0.3767794668674469\n","Batch  43  Loss  0.37566664814949036\n","Batch  44  Loss  0.3978506922721863\n","Batch  45  Loss  0.38034528493881226\n","Batch  46  Loss  0.38863810896873474\n","Batch  47  Loss  0.3903995454311371\n","Batch  48  Loss  0.38180163502693176\n","Batch  49  Loss  0.3852614760398865\n","Batch  50  Loss  0.37887200713157654\n","Batch  51  Loss  0.3778741955757141\n","Batch  52  Loss  0.3840027153491974\n","Batch  53  Loss  0.38253000378608704\n","Batch  54  Loss  0.38044050335884094\n","Batch  55  Loss  0.3731894791126251\n","Batch  56  Loss  0.3697962760925293\n","Batch  57  Loss  0.3801511526107788\n","Batch  58  Loss  0.38889849185943604\n","Batch  59  Loss  0.3690044581890106\n","Batch  60  Loss  0.37352168560028076\n","Batch  61  Loss  0.37899109721183777\n","Batch  62  Loss  0.3765176236629486\n","Batch  63  Loss  0.3676556646823883\n","Batch  64  Loss  0.3804638683795929\n","Batch  65  Loss  0.36108991503715515\n","Batch  66  Loss  0.3662364184856415\n","Batch  67  Loss  0.38873952627182007\n","Batch  68  Loss  0.36818355321884155\n","Batch  69  Loss  0.365213543176651\n","Batch  70  Loss  0.3792954087257385\n","Batch  71  Loss  0.3823660910129547\n","Batch  72  Loss  0.37132859230041504\n","Batch  73  Loss  0.38424643874168396\n","Batch  74  Loss  0.3900647461414337\n","Batch  75  Loss  0.3808363974094391\n","Batch  76  Loss  0.37142014503479004\n","Batch  77  Loss  0.3779281675815582\n","Batch  78  Loss  0.371086448431015\n","Batch  79  Loss  0.37137770652770996\n","Batch  80  Loss  0.36066967248916626\n","Batch  81  Loss  0.37709102034568787\n","Batch  82  Loss  0.383661687374115\n","Batch  83  Loss  0.37553560733795166\n","Batch  84  Loss  0.3927370309829712\n","Batch  85  Loss  0.36716315150260925\n","Batch  86  Loss  0.372101366519928\n","Batch  87  Loss  0.39601486921310425\n","Batch  88  Loss  0.3780571222305298\n","Batch  89  Loss  0.373692125082016\n","Batch  90  Loss  0.3745439648628235\n","Batch  91  Loss  0.36995789408683777\n","Batch  92  Loss  0.37926822900772095\n","Batch  93  Loss  0.37998151779174805\n","Batch  94  Loss  0.37538230419158936\n","Batch  95  Loss  0.3841646909713745\n","Batch  96  Loss  0.3898184299468994\n","Batch  97  Loss  0.3705534338951111\n","Batch  98  Loss  0.38329774141311646\n","Batch  99  Loss  0.38015055656433105\n","Batch  100  Loss  0.3670125901699066\n","Batch  101  Loss  0.37619733810424805\n","Batch  102  Loss  0.37424471974372864\n","Batch  103  Loss  0.38125231862068176\n","Batch  104  Loss  0.37340983748435974\n","Batch  105  Loss  0.3717398941516876\n","Batch  106  Loss  0.36439087986946106\n","Batch  107  Loss  0.3706198036670685\n","Batch  108  Loss  0.3822931945323944\n","Batch  109  Loss  0.3584895133972168\n","Batch  110  Loss  0.3635932505130768\n","Batch  111  Loss  0.36452171206474304\n","Batch  112  Loss  0.36784905195236206\n","Batch  113  Loss  0.3783436715602875\n","Batch  114  Loss  0.3775649964809418\n","Batch  115  Loss  0.3802608847618103\n","Batch  116  Loss  0.36963000893592834\n","Batch  117  Loss  0.35979682207107544\n","Batch  118  Loss  0.36046507954597473\n","Batch  119  Loss  0.3708841800689697\n","Batch  120  Loss  0.36751794815063477\n","Batch  121  Loss  0.3669675290584564\n","Batch  122  Loss  0.385138601064682\n","Batch  123  Loss  0.36945977807044983\n","Batch  124  Loss  0.3708825409412384\n","Batch  125  Loss  0.38244563341140747\n","Batch  126  Loss  0.3833947777748108\n","Batch  127  Loss  0.3799220025539398\n","Batch  128  Loss  0.3562498688697815\n","Batch  129  Loss  0.3656247556209564\n","Batch  130  Loss  0.3710468113422394\n","Batch  131  Loss  0.36987701058387756\n","Batch  132  Loss  0.3828507661819458\n","Batch  133  Loss  0.39648064970970154\n","Batch  134  Loss  0.3844210207462311\n","Batch  135  Loss  0.36427852511405945\n","Batch  136  Loss  0.36669525504112244\n","Batch  137  Loss  0.38147401809692383\n","Batch  138  Loss  0.3755996823310852\n","Batch  139  Loss  0.38395956158638\n","Batch  140  Loss  0.3648175597190857\n","Batch  141  Loss  0.367559552192688\n","Batch  142  Loss  0.3673994243144989\n","Batch  143  Loss  0.3827267289161682\n","Batch  144  Loss  0.35870787501335144\n","Batch  145  Loss  0.38473764061927795\n","Batch  146  Loss  0.383658766746521\n","Batch  147  Loss  0.385786771774292\n","Batch  148  Loss  0.37618082761764526\n","Batch  149  Loss  0.3739446699619293\n","Batch  150  Loss  0.3692653775215149\n","Batch  151  Loss  0.3868618905544281\n","Batch  152  Loss  0.35811516642570496\n","Batch  153  Loss  0.37577053904533386\n","Batch  154  Loss  0.3686329126358032\n","Batch  155  Loss  0.3671679198741913\n","Batch  156  Loss  0.3684171438217163\n","Batch  157  Loss  0.37698686122894287\n","Batch  158  Loss  0.38072243332862854\n","Batch  159  Loss  0.37272387742996216\n","Batch  160  Loss  0.37177804112434387\n","Batch  161  Loss  0.37619972229003906\n","Batch  162  Loss  0.37109023332595825\n","Batch  163  Loss  0.3731122314929962\n","Batch  164  Loss  0.36258095502853394\n","Batch  165  Loss  0.3902338445186615\n","Epoch: 1 , Loss: tensor(0.0615, device='cuda:0', grad_fn=<AddBackward0>)\n","Batch  1  Loss  0.3779709041118622\n","Batch  2  Loss  0.38079047203063965\n","Batch  3  Loss  0.37261906266212463\n","Batch  4  Loss  0.37766337394714355\n","Batch  5  Loss  0.3951493501663208\n","Batch  6  Loss  0.37181687355041504\n","Batch  7  Loss  0.3929266333580017\n","Batch  8  Loss  0.37197455763816833\n","Batch  9  Loss  0.3787502646446228\n","Batch  10  Loss  0.37928611040115356\n","Batch  11  Loss  0.3743966519832611\n","Batch  12  Loss  0.3658621907234192\n","Batch  13  Loss  0.3873484134674072\n","Batch  14  Loss  0.375948965549469\n","Batch  15  Loss  0.37530437111854553\n","Batch  16  Loss  0.3740571141242981\n","Batch  17  Loss  0.3784804046154022\n","Batch  18  Loss  0.3782688081264496\n","Batch  19  Loss  0.35509639978408813\n","Batch  20  Loss  0.3589158058166504\n","Batch  21  Loss  0.37506967782974243\n","Batch  22  Loss  0.38470956683158875\n","Batch  23  Loss  0.37365713715553284\n","Batch  24  Loss  0.376766562461853\n","Batch  25  Loss  0.35579466819763184\n","Batch  26  Loss  0.36587417125701904\n","Batch  27  Loss  0.37889423966407776\n","Batch  28  Loss  0.36394596099853516\n","Batch  29  Loss  0.3674732446670532\n","Batch  30  Loss  0.3635696768760681\n","Batch  31  Loss  0.3689235746860504\n","Batch  32  Loss  0.3808489739894867\n","Batch  33  Loss  0.3838095963001251\n","Batch  34  Loss  0.3719995319843292\n","Batch  35  Loss  0.3721897602081299\n","Batch  36  Loss  0.3732399344444275\n","Batch  37  Loss  0.36572661995887756\n","Batch  38  Loss  0.35709258913993835\n","Batch  39  Loss  0.36131465435028076\n","Batch  40  Loss  0.3677394688129425\n","Batch  41  Loss  0.3636587858200073\n","Batch  42  Loss  0.3647070825099945\n","Batch  43  Loss  0.3679068386554718\n","Batch  44  Loss  0.3595947325229645\n","Batch  45  Loss  0.37476447224617004\n","Batch  46  Loss  0.3669085204601288\n","Batch  47  Loss  0.37739214301109314\n","Batch  48  Loss  0.3662557899951935\n","Batch  49  Loss  0.3774697780609131\n","Batch  50  Loss  0.3482333719730377\n","Batch  51  Loss  0.3647902011871338\n","Batch  52  Loss  0.36740919947624207\n","Batch  53  Loss  0.36125174164772034\n","Batch  54  Loss  0.35886868834495544\n","Batch  55  Loss  0.366818368434906\n","Batch  56  Loss  0.3831108510494232\n","Batch  57  Loss  0.3714069426059723\n","Batch  58  Loss  0.36299824714660645\n","Batch  59  Loss  0.3725317120552063\n","Batch  60  Loss  0.3752758204936981\n","Batch  61  Loss  0.3631677031517029\n","Batch  62  Loss  0.35382741689682007\n","Batch  63  Loss  0.3663768172264099\n","Batch  64  Loss  0.3621633052825928\n","Batch  65  Loss  0.3666422963142395\n","Batch  66  Loss  0.37946057319641113\n","Batch  67  Loss  0.36030682921409607\n","Batch  68  Loss  0.3698868453502655\n","Batch  69  Loss  0.36797356605529785\n","Batch  70  Loss  0.3726233243942261\n","Batch  71  Loss  0.36910760402679443\n","Batch  72  Loss  0.37043315172195435\n","Batch  73  Loss  0.3685598075389862\n","Batch  74  Loss  0.3573991358280182\n","Batch  75  Loss  0.3660636842250824\n","Batch  76  Loss  0.3684641122817993\n","Batch  77  Loss  0.3561926484107971\n","Batch  78  Loss  0.35963717103004456\n","Batch  79  Loss  0.35655444860458374\n","Batch  80  Loss  0.3484940528869629\n","Batch  81  Loss  0.3673113286495209\n","Batch  82  Loss  0.3621963858604431\n","Batch  83  Loss  0.3662582337856293\n","Batch  84  Loss  0.37246546149253845\n","Batch  85  Loss  0.3515045642852783\n","Batch  86  Loss  0.3562517762184143\n","Batch  87  Loss  0.3565446734428406\n","Batch  88  Loss  0.3643556535243988\n","Batch  89  Loss  0.3638637661933899\n","Batch  90  Loss  0.35529381036758423\n","Batch  91  Loss  0.36010128259658813\n","Batch  92  Loss  0.37034210562705994\n","Batch  93  Loss  0.36527660489082336\n","Batch  94  Loss  0.3579265773296356\n","Batch  95  Loss  0.3818226456642151\n","Batch  96  Loss  0.37195470929145813\n","Batch  97  Loss  0.3697846829891205\n","Batch  98  Loss  0.3750191628932953\n","Batch  99  Loss  0.35157132148742676\n","Batch  100  Loss  0.36892953515052795\n","Batch  101  Loss  0.3497207760810852\n","Batch  102  Loss  0.3640109896659851\n","Batch  103  Loss  0.38025668263435364\n","Batch  104  Loss  0.3662736713886261\n","Batch  105  Loss  0.3470114469528198\n","Batch  106  Loss  0.37449201941490173\n","Batch  107  Loss  0.3600337505340576\n","Batch  108  Loss  0.36579257249832153\n","Batch  109  Loss  0.3658974766731262\n","Batch  110  Loss  0.37136924266815186\n","Batch  111  Loss  0.37250152230262756\n","Batch  112  Loss  0.3624286651611328\n","Batch  113  Loss  0.35666248202323914\n","Batch  114  Loss  0.3645842969417572\n","Batch  115  Loss  0.3717426657676697\n","Batch  116  Loss  0.3744316101074219\n","Batch  117  Loss  0.3588074743747711\n","Batch  118  Loss  0.3754277229309082\n","Batch  119  Loss  0.36229953169822693\n","Batch  120  Loss  0.37431275844573975\n","Batch  121  Loss  0.3610069453716278\n","Batch  122  Loss  0.3784470558166504\n","Batch  123  Loss  0.36465397477149963\n","Batch  124  Loss  0.3516780138015747\n","Batch  125  Loss  0.3789156675338745\n","Batch  126  Loss  0.363892138004303\n","Batch  127  Loss  0.3514314889907837\n","Batch  128  Loss  0.3649272620677948\n","Batch  129  Loss  0.3589347004890442\n","Batch  130  Loss  0.3517373204231262\n","Batch  131  Loss  0.34665587544441223\n","Batch  132  Loss  0.37169021368026733\n","Batch  133  Loss  0.36226487159729004\n","Batch  134  Loss  0.3621281385421753\n","Batch  135  Loss  0.37315496802330017\n","Batch  136  Loss  0.35831305384635925\n","Batch  137  Loss  0.36959657073020935\n","Batch  138  Loss  0.3723067343235016\n","Batch  139  Loss  0.3727450370788574\n","Batch  140  Loss  0.37163862586021423\n","Batch  141  Loss  0.35674601793289185\n","Batch  142  Loss  0.36600860953330994\n","Batch  143  Loss  0.3575434386730194\n","Batch  144  Loss  0.37460610270500183\n","Batch  145  Loss  0.36277857422828674\n","Batch  146  Loss  0.3715413808822632\n","Batch  147  Loss  0.37813234329223633\n","Batch  148  Loss  0.36755990982055664\n","Batch  149  Loss  0.3466704785823822\n","Batch  150  Loss  0.35660818219184875\n","Batch  151  Loss  0.3554427921772003\n","Batch  152  Loss  0.3682793080806732\n","Batch  153  Loss  0.37801989912986755\n","Batch  154  Loss  0.3619080185890198\n","Batch  155  Loss  0.3607293665409088\n","Batch  156  Loss  0.3615463972091675\n","Batch  157  Loss  0.3676280975341797\n","Batch  158  Loss  0.3665550649166107\n","Batch  159  Loss  0.37669652700424194\n","Batch  160  Loss  0.34733739495277405\n","Batch  161  Loss  0.36315494775772095\n","Batch  162  Loss  0.35965514183044434\n","Batch  163  Loss  0.3603774309158325\n","Batch  164  Loss  0.3653223216533661\n","Batch  165  Loss  0.3732316792011261\n","Epoch: 2 , Loss: tensor(0.0595, device='cuda:0', grad_fn=<AddBackward0>)\n","Batch  1  Loss  0.347043514251709\n","Batch  2  Loss  0.3590690493583679\n","Batch  3  Loss  0.3549538552761078\n","Batch  4  Loss  0.36940324306488037\n","Batch  5  Loss  0.35255488753318787\n","Batch  6  Loss  0.37025997042655945\n","Batch  7  Loss  0.36389049887657166\n","Batch  8  Loss  0.3614144027233124\n","Batch  9  Loss  0.3748589754104614\n","Batch  10  Loss  0.3539975583553314\n","Batch  11  Loss  0.3598663806915283\n","Batch  12  Loss  0.3501565456390381\n","Batch  13  Loss  0.3621028959751129\n","Batch  14  Loss  0.36690422892570496\n","Batch  15  Loss  0.36518773436546326\n","Batch  16  Loss  0.3636855483055115\n","Batch  17  Loss  0.3651922643184662\n","Batch  18  Loss  0.37942132353782654\n","Batch  19  Loss  0.368596613407135\n","Batch  20  Loss  0.3701218366622925\n","Batch  21  Loss  0.35433200001716614\n","Batch  22  Loss  0.3451892137527466\n","Batch  23  Loss  0.3753626048564911\n","Batch  24  Loss  0.35705265402793884\n","Batch  25  Loss  0.3653947412967682\n","Batch  26  Loss  0.36199063062667847\n","Batch  27  Loss  0.3656217157840729\n","Batch  28  Loss  0.36982235312461853\n","Batch  29  Loss  0.371009886264801\n","Batch  30  Loss  0.36359256505966187\n","Batch  31  Loss  0.35290414094924927\n","Batch  32  Loss  0.3761574923992157\n","Batch  33  Loss  0.36354032158851624\n","Batch  34  Loss  0.3666452169418335\n","Batch  35  Loss  0.3582797348499298\n","Batch  36  Loss  0.35736724734306335\n","Batch  37  Loss  0.35785913467407227\n","Batch  38  Loss  0.3660091161727905\n","Batch  39  Loss  0.3705575466156006\n","Batch  40  Loss  0.3654332458972931\n","Batch  41  Loss  0.3629921078681946\n","Batch  42  Loss  0.35813936591148376\n","Batch  43  Loss  0.3572603762149811\n","Batch  44  Loss  0.36981862783432007\n","Batch  45  Loss  0.36331090331077576\n","Batch  46  Loss  0.37059828639030457\n","Batch  47  Loss  0.3590160310268402\n","Batch  48  Loss  0.36533626914024353\n","Batch  49  Loss  0.3626604676246643\n","Batch  50  Loss  0.3621402978897095\n","Batch  51  Loss  0.36318159103393555\n","Batch  52  Loss  0.355285108089447\n","Batch  53  Loss  0.36015668511390686\n","Batch  54  Loss  0.3555859327316284\n","Batch  55  Loss  0.3551228940486908\n","Batch  56  Loss  0.3689165413379669\n","Batch  57  Loss  0.3612246811389923\n","Batch  58  Loss  0.35589030385017395\n","Batch  59  Loss  0.34993046522140503\n","Batch  60  Loss  0.3643319606781006\n","Batch  61  Loss  0.3620595633983612\n","Batch  62  Loss  0.37306836247444153\n","Batch  63  Loss  0.34721699357032776\n","Batch  64  Loss  0.35925254225730896\n","Batch  65  Loss  0.35099101066589355\n","Batch  66  Loss  0.3594079613685608\n","Batch  67  Loss  0.3589138388633728\n","Batch  68  Loss  0.3683844208717346\n","Batch  69  Loss  0.36144331097602844\n","Batch  70  Loss  0.3606787323951721\n","Batch  71  Loss  0.3645434081554413\n","Batch  72  Loss  0.3536171317100525\n","Batch  73  Loss  0.3572753369808197\n","Batch  74  Loss  0.37484559416770935\n","Batch  75  Loss  0.36089855432510376\n","Batch  76  Loss  0.36397141218185425\n","Batch  77  Loss  0.3524434268474579\n","Batch  78  Loss  0.3594153821468353\n","Batch  79  Loss  0.35421475768089294\n","Batch  80  Loss  0.34540078043937683\n","Batch  81  Loss  0.345906525850296\n","Batch  82  Loss  0.3607737123966217\n","Batch  83  Loss  0.34495019912719727\n","Batch  84  Loss  0.3578069806098938\n","Batch  85  Loss  0.3459671139717102\n","Batch  86  Loss  0.3614625930786133\n","Batch  87  Loss  0.36417216062545776\n","Batch  88  Loss  0.3534621596336365\n","Batch  89  Loss  0.3681047558784485\n","Batch  90  Loss  0.35547035932540894\n","Batch  91  Loss  0.36437875032424927\n","Batch  92  Loss  0.34843891859054565\n","Batch  93  Loss  0.36283445358276367\n","Batch  94  Loss  0.3577287495136261\n","Batch  95  Loss  0.35995566844940186\n","Batch  96  Loss  0.3535703718662262\n","Batch  97  Loss  0.3629242479801178\n","Batch  98  Loss  0.35062989592552185\n","Batch  99  Loss  0.369548499584198\n","Batch  100  Loss  0.36621084809303284\n","Batch  101  Loss  0.37171855568885803\n","Batch  102  Loss  0.3513109087944031\n","Batch  103  Loss  0.3597879111766815\n","Batch  104  Loss  0.3656519055366516\n","Batch  105  Loss  0.3482140600681305\n","Batch  106  Loss  0.3677038252353668\n","Batch  107  Loss  0.3499130308628082\n","Batch  108  Loss  0.35650721192359924\n","Batch  109  Loss  0.3500133156776428\n","Batch  110  Loss  0.35377517342567444\n","Batch  111  Loss  0.3493478000164032\n","Batch  112  Loss  0.3548658490180969\n","Batch  113  Loss  0.37156808376312256\n","Batch  114  Loss  0.36061832308769226\n","Batch  115  Loss  0.35596218705177307\n","Batch  116  Loss  0.36135274171829224\n","Batch  117  Loss  0.3575270473957062\n","Batch  118  Loss  0.35962727665901184\n","Batch  119  Loss  0.35948407649993896\n","Batch  120  Loss  0.36652299761772156\n","Batch  121  Loss  0.34828880429267883\n","Batch  122  Loss  0.3619967997074127\n","Batch  123  Loss  0.3523809611797333\n","Batch  124  Loss  0.3565569221973419\n","Batch  125  Loss  0.35180017352104187\n","Batch  126  Loss  0.3471376299858093\n","Batch  127  Loss  0.34525129199028015\n","Batch  128  Loss  0.36755701899528503\n","Batch  129  Loss  0.3464512825012207\n","Batch  130  Loss  0.3422798812389374\n","Batch  131  Loss  0.3558557331562042\n","Batch  132  Loss  0.3452528715133667\n","Batch  133  Loss  0.36714011430740356\n","Batch  134  Loss  0.3686571717262268\n","Batch  135  Loss  0.36582791805267334\n","Batch  136  Loss  0.3551101088523865\n","Batch  137  Loss  0.3652099370956421\n","Batch  138  Loss  0.3568131625652313\n","Batch  139  Loss  0.36018073558807373\n","Batch  140  Loss  0.34111708402633667\n","Batch  141  Loss  0.376473993062973\n","Batch  142  Loss  0.36476171016693115\n","Batch  143  Loss  0.36440056562423706\n","Batch  144  Loss  0.3619879186153412\n","Batch  145  Loss  0.35741621255874634\n","Batch  146  Loss  0.3538188338279724\n","Batch  147  Loss  0.35937216877937317\n","Batch  148  Loss  0.36761265993118286\n","Batch  149  Loss  0.36077606678009033\n","Batch  150  Loss  0.35926738381385803\n","Batch  151  Loss  0.3398612141609192\n","Batch  152  Loss  0.351494163274765\n","Batch  153  Loss  0.35724443197250366\n","Batch  154  Loss  0.3706771731376648\n","Batch  155  Loss  0.3656047284603119\n","Batch  156  Loss  0.3540722727775574\n","Batch  157  Loss  0.3643198013305664\n","Batch  158  Loss  0.35567042231559753\n","Batch  159  Loss  0.36252689361572266\n","Batch  160  Loss  0.3438483476638794\n","Batch  161  Loss  0.3572641909122467\n","Batch  162  Loss  0.3529864549636841\n","Batch  163  Loss  0.36741262674331665\n","Batch  164  Loss  0.35794830322265625\n","Batch  165  Loss  0.33876273036003113\n","Epoch: 3 , Loss: tensor(0.0582, device='cuda:0', grad_fn=<AddBackward0>)\n","Batch  1  Loss  0.3577931821346283\n","Batch  2  Loss  0.35310491919517517\n","Batch  3  Loss  0.3548959493637085\n","Batch  4  Loss  0.3556991219520569\n","Batch  5  Loss  0.36212316155433655\n","Batch  6  Loss  0.36175575852394104\n","Batch  7  Loss  0.35322436690330505\n","Batch  8  Loss  0.35925790667533875\n","Batch  9  Loss  0.35350123047828674\n","Batch  10  Loss  0.36251720786094666\n","Batch  11  Loss  0.3566683828830719\n","Batch  12  Loss  0.3579466938972473\n","Batch  13  Loss  0.3664487600326538\n","Batch  14  Loss  0.3574672043323517\n","Batch  15  Loss  0.3593215346336365\n","Batch  16  Loss  0.3574914038181305\n","Batch  17  Loss  0.3459435999393463\n","Batch  18  Loss  0.34404274821281433\n","Batch  19  Loss  0.34809887409210205\n","Batch  20  Loss  0.3614869713783264\n","Batch  21  Loss  0.36114218831062317\n","Batch  22  Loss  0.34526968002319336\n","Batch  23  Loss  0.349346399307251\n","Batch  24  Loss  0.3650180697441101\n","Batch  25  Loss  0.34601718187332153\n","Batch  26  Loss  0.3607248365879059\n","Batch  27  Loss  0.3355756402015686\n","Batch  28  Loss  0.36570650339126587\n","Batch  29  Loss  0.3573940098285675\n","Batch  30  Loss  0.34414198994636536\n","Batch  31  Loss  0.34435516595840454\n","Batch  32  Loss  0.3593079149723053\n","Batch  33  Loss  0.3494173586368561\n","Batch  34  Loss  0.3575998842716217\n","Batch  35  Loss  0.3361494541168213\n","Batch  36  Loss  0.34104254841804504\n","Batch  37  Loss  0.3678266108036041\n","Batch  38  Loss  0.3404487073421478\n","Batch  39  Loss  0.35158559679985046\n","Batch  40  Loss  0.3528852164745331\n","Batch  41  Loss  0.3543145954608917\n","Batch  42  Loss  0.3534613251686096\n","Batch  43  Loss  0.3525911867618561\n","Batch  44  Loss  0.3428431749343872\n","Batch  45  Loss  0.35786813497543335\n","Batch  46  Loss  0.35865920782089233\n","Batch  47  Loss  0.3587617874145508\n","Batch  48  Loss  0.3521169126033783\n","Batch  49  Loss  0.345689594745636\n","Batch  50  Loss  0.34429657459259033\n","Batch  51  Loss  0.34823817014694214\n","Batch  52  Loss  0.3548516035079956\n","Batch  53  Loss  0.3460267186164856\n","Batch  54  Loss  0.35009995102882385\n","Batch  55  Loss  0.3475131094455719\n","Batch  56  Loss  0.35337114334106445\n","Batch  57  Loss  0.3489665389060974\n","Batch  58  Loss  0.3507694900035858\n","Batch  59  Loss  0.3587174713611603\n","Batch  60  Loss  0.34234151244163513\n","Batch  61  Loss  0.3461509346961975\n","Batch  62  Loss  0.3441309928894043\n","Batch  63  Loss  0.36547887325286865\n","Batch  64  Loss  0.35123246908187866\n","Batch  65  Loss  0.346333771944046\n","Batch  66  Loss  0.3576838970184326\n","Batch  67  Loss  0.3486558496952057\n","Batch  68  Loss  0.35625746846199036\n","Batch  69  Loss  0.34945425391197205\n","Batch  70  Loss  0.34336182475090027\n","Batch  71  Loss  0.34415000677108765\n","Batch  72  Loss  0.3509427011013031\n","Batch  73  Loss  0.34851962327957153\n","Batch  74  Loss  0.33731532096862793\n","Batch  75  Loss  0.3521987497806549\n","Batch  76  Loss  0.33582603931427\n","Batch  77  Loss  0.35259291529655457\n","Batch  78  Loss  0.3538302481174469\n","Batch  79  Loss  0.3620469868183136\n","Batch  80  Loss  0.3418601453304291\n","Batch  81  Loss  0.34663212299346924\n","Batch  82  Loss  0.34060534834861755\n","Batch  83  Loss  0.3561616539955139\n","Batch  84  Loss  0.347072035074234\n","Batch  85  Loss  0.3408999741077423\n","Batch  86  Loss  0.3408306837081909\n","Batch  87  Loss  0.3401568830013275\n","Batch  88  Loss  0.34116214513778687\n","Batch  89  Loss  0.3414967656135559\n","Batch  90  Loss  0.36616870760917664\n","Batch  91  Loss  0.3546094000339508\n","Batch  92  Loss  0.34087705612182617\n","Batch  93  Loss  0.3598785102367401\n","Batch  94  Loss  0.35028699040412903\n","Batch  95  Loss  0.34976422786712646\n","Batch  96  Loss  0.34795400500297546\n","Batch  97  Loss  0.325655460357666\n","Batch  98  Loss  0.3386511206626892\n","Batch  99  Loss  0.3412669599056244\n","Batch  100  Loss  0.34492868185043335\n","Batch  101  Loss  0.3394218683242798\n","Batch  102  Loss  0.35817256569862366\n","Batch  103  Loss  0.35922375321388245\n","Batch  104  Loss  0.36140456795692444\n","Batch  105  Loss  0.34820792078971863\n","Batch  106  Loss  0.34446486830711365\n","Batch  107  Loss  0.34879422187805176\n","Batch  108  Loss  0.340053528547287\n","Batch  109  Loss  0.32971832156181335\n","Batch  110  Loss  0.33344149589538574\n","Batch  111  Loss  0.34176838397979736\n","Batch  112  Loss  0.3486920893192291\n","Batch  113  Loss  0.3623602092266083\n","Batch  114  Loss  0.3320799171924591\n","Batch  115  Loss  0.33567318320274353\n","Batch  116  Loss  0.3562692403793335\n","Batch  117  Loss  0.3493351638317108\n","Batch  118  Loss  0.34056374430656433\n","Batch  119  Loss  0.3459542393684387\n","Batch  120  Loss  0.33066701889038086\n","Batch  121  Loss  0.3433009386062622\n","Batch  122  Loss  0.33922508358955383\n","Batch  123  Loss  0.34642112255096436\n","Batch  124  Loss  0.3626508414745331\n","Batch  125  Loss  0.34474480152130127\n","Batch  126  Loss  0.34553077816963196\n","Batch  127  Loss  0.3381393253803253\n","Batch  128  Loss  0.35248270630836487\n","Batch  129  Loss  0.3463488519191742\n","Batch  130  Loss  0.34774652123451233\n","Batch  131  Loss  0.3461100161075592\n","Batch  132  Loss  0.34135764837265015\n","Batch  133  Loss  0.3597877025604248\n","Batch  134  Loss  0.33199623227119446\n","Batch  135  Loss  0.350541353225708\n","Batch  136  Loss  0.35285574197769165\n","Batch  137  Loss  0.33761218190193176\n","Batch  138  Loss  0.3604850769042969\n","Batch  139  Loss  0.3480691611766815\n","Batch  140  Loss  0.3620836138725281\n","Batch  141  Loss  0.34727978706359863\n","Batch  142  Loss  0.3436548411846161\n","Batch  143  Loss  0.35016804933547974\n","Batch  144  Loss  0.3402428925037384\n","Batch  145  Loss  0.35453495383262634\n","Batch  146  Loss  0.33875906467437744\n","Batch  147  Loss  0.34393033385276794\n","Batch  148  Loss  0.34078386425971985\n","Batch  149  Loss  0.3516365885734558\n","Batch  150  Loss  0.35244888067245483\n","Batch  151  Loss  0.3466532826423645\n","Batch  152  Loss  0.34809717535972595\n","Batch  153  Loss  0.341465026140213\n","Batch  154  Loss  0.3347850441932678\n","Batch  155  Loss  0.35360386967658997\n","Batch  156  Loss  0.3468649387359619\n","Batch  157  Loss  0.3331536650657654\n","Batch  158  Loss  0.34753698110580444\n","Batch  159  Loss  0.34463977813720703\n","Batch  160  Loss  0.3465502858161926\n","Batch  161  Loss  0.34194934368133545\n","Batch  162  Loss  0.3459973931312561\n","Batch  163  Loss  0.33146724104881287\n","Batch  164  Loss  0.3418564200401306\n","Batch  165  Loss  0.36280038952827454\n","Epoch: 4 , Loss: tensor(0.0566, device='cuda:0', grad_fn=<AddBackward0>)\n","Batch  1  Loss  0.343923956155777\n","Batch  2  Loss  0.33818134665489197\n","Batch  3  Loss  0.35528117418289185\n","Batch  4  Loss  0.35139212012290955\n","Batch  5  Loss  0.3477323651313782\n","Batch  6  Loss  0.3455022871494293\n","Batch  7  Loss  0.34454554319381714\n","Batch  8  Loss  0.3474824130535126\n","Batch  9  Loss  0.34493571519851685\n","Batch  10  Loss  0.3432273268699646\n","Batch  11  Loss  0.341691792011261\n","Batch  12  Loss  0.34170788526535034\n","Batch  13  Loss  0.3465177118778229\n","Batch  14  Loss  0.3536525368690491\n","Batch  15  Loss  0.3481369912624359\n","Batch  16  Loss  0.34685561060905457\n","Batch  17  Loss  0.34734976291656494\n","Batch  18  Loss  0.3436950147151947\n","Batch  19  Loss  0.3379194438457489\n","Batch  20  Loss  0.3444293737411499\n","Batch  21  Loss  0.3460228741168976\n","Batch  22  Loss  0.3503808379173279\n","Batch  23  Loss  0.34646984934806824\n","Batch  24  Loss  0.34253010153770447\n","Batch  25  Loss  0.3371083736419678\n","Batch  26  Loss  0.3304601311683655\n","Batch  27  Loss  0.3340805470943451\n","Batch  28  Loss  0.3361737132072449\n","Batch  29  Loss  0.3412954807281494\n","Batch  30  Loss  0.34066110849380493\n","Batch  31  Loss  0.3356015086174011\n","Batch  32  Loss  0.3345411419868469\n","Batch  33  Loss  0.34069204330444336\n","Batch  34  Loss  0.3360525369644165\n","Batch  35  Loss  0.35274210572242737\n","Batch  36  Loss  0.34393104910850525\n","Batch  37  Loss  0.3256518244743347\n","Batch  38  Loss  0.34013059735298157\n","Batch  39  Loss  0.34264299273490906\n","Batch  40  Loss  0.33706575632095337\n","Batch  41  Loss  0.33691439032554626\n","Batch  42  Loss  0.32371872663497925\n","Batch  43  Loss  0.3358120918273926\n","Batch  44  Loss  0.33881720900535583\n","Batch  45  Loss  0.3277650475502014\n","Batch  46  Loss  0.3434409499168396\n","Batch  47  Loss  0.3311498463153839\n","Batch  48  Loss  0.32586589455604553\n","Batch  49  Loss  0.34051674604415894\n","Batch  50  Loss  0.33809784054756165\n","Batch  51  Loss  0.3326781690120697\n","Batch  52  Loss  0.34162238240242004\n","Batch  53  Loss  0.32980236411094666\n","Batch  54  Loss  0.3340086042881012\n","Batch  55  Loss  0.32471996545791626\n","Batch  56  Loss  0.3303525745868683\n","Batch  57  Loss  0.3468121290206909\n","Batch  58  Loss  0.34197208285331726\n","Batch  59  Loss  0.32358846068382263\n","Batch  60  Loss  0.33042722940444946\n","Batch  61  Loss  0.3421032428741455\n","Batch  62  Loss  0.3303213119506836\n","Batch  63  Loss  0.33441659808158875\n","Batch  64  Loss  0.3329312205314636\n","Batch  65  Loss  0.3300507366657257\n","Batch  66  Loss  0.3194608986377716\n","Batch  67  Loss  0.32924872636795044\n","Batch  68  Loss  0.3276638090610504\n","Batch  69  Loss  0.3399336338043213\n","Batch  70  Loss  0.3207263648509979\n","Batch  71  Loss  0.32563650608062744\n","Batch  72  Loss  0.31998682022094727\n","Batch  73  Loss  0.315640926361084\n","Batch  74  Loss  0.3369811773300171\n","Batch  75  Loss  0.3179396390914917\n","Batch  76  Loss  0.3300178349018097\n","Batch  77  Loss  0.3316432535648346\n","Batch  78  Loss  0.3282668888568878\n","Batch  79  Loss  0.3449867367744446\n","Batch  80  Loss  0.33386871218681335\n","Batch  81  Loss  0.3268812298774719\n","Batch  82  Loss  0.31970399618148804\n","Batch  83  Loss  0.3329687714576721\n","Batch  84  Loss  0.3258205056190491\n","Batch  85  Loss  0.32533538341522217\n","Batch  86  Loss  0.31999221444129944\n","Batch  87  Loss  0.31697577238082886\n","Batch  88  Loss  0.32081338763237\n","Batch  89  Loss  0.3298056125640869\n","Batch  90  Loss  0.33257848024368286\n","Batch  91  Loss  0.3290260136127472\n","Batch  92  Loss  0.3323148787021637\n","Batch  93  Loss  0.3316941261291504\n","Batch  94  Loss  0.3271525502204895\n","Batch  95  Loss  0.32859790325164795\n","Batch  96  Loss  0.3278525769710541\n","Batch  97  Loss  0.3349653482437134\n","Batch  98  Loss  0.31964102387428284\n","Batch  99  Loss  0.3313019573688507\n","Batch  100  Loss  0.3201752305030823\n","Batch  101  Loss  0.33407601714134216\n","Batch  102  Loss  0.31694790720939636\n","Batch  103  Loss  0.3189455568790436\n","Batch  104  Loss  0.3315468430519104\n","Batch  105  Loss  0.3198411464691162\n","Batch  106  Loss  0.3360368013381958\n","Batch  107  Loss  0.33604949712753296\n","Batch  108  Loss  0.32586798071861267\n","Batch  109  Loss  0.32750481367111206\n","Batch  110  Loss  0.3178066611289978\n","Batch  111  Loss  0.32356923818588257\n","Batch  112  Loss  0.3407154679298401\n","Batch  113  Loss  0.33144620060920715\n","Batch  114  Loss  0.32955417037010193\n","Batch  115  Loss  0.32301896810531616\n","Batch  116  Loss  0.3221632242202759\n","Batch  117  Loss  0.32436177134513855\n","Batch  118  Loss  0.327659010887146\n","Batch  119  Loss  0.33488667011260986\n","Batch  120  Loss  0.3259517252445221\n","Batch  121  Loss  0.3215923309326172\n","Batch  122  Loss  0.3277962803840637\n","Batch  123  Loss  0.32816633582115173\n","Batch  124  Loss  0.3252077102661133\n","Batch  125  Loss  0.3276020288467407\n","Batch  126  Loss  0.3326431214809418\n","Batch  127  Loss  0.32440969347953796\n","Batch  128  Loss  0.32237955927848816\n","Batch  129  Loss  0.32509830594062805\n","Batch  130  Loss  0.3238467574119568\n","Batch  131  Loss  0.3201211094856262\n","Batch  132  Loss  0.31765004992485046\n","Batch  133  Loss  0.3262626826763153\n","Batch  134  Loss  0.3217393457889557\n","Batch  135  Loss  0.3206062316894531\n","Batch  136  Loss  0.32415878772735596\n","Batch  137  Loss  0.329765260219574\n","Batch  138  Loss  0.3295117914676666\n","Batch  139  Loss  0.3283633291721344\n","Batch  140  Loss  0.33336514234542847\n","Batch  141  Loss  0.3251446783542633\n","Batch  142  Loss  0.33546286821365356\n","Batch  143  Loss  0.32676267623901367\n","Batch  144  Loss  0.33048534393310547\n","Batch  145  Loss  0.3169296085834503\n","Batch  146  Loss  0.33696988224983215\n","Batch  147  Loss  0.3184658885002136\n","Batch  148  Loss  0.33343014121055603\n","Batch  149  Loss  0.3323614299297333\n","Batch  150  Loss  0.3197295665740967\n","Batch  151  Loss  0.3264605700969696\n","Batch  152  Loss  0.3310950696468353\n","Batch  153  Loss  0.3311161994934082\n","Batch  154  Loss  0.3274271786212921\n","Batch  155  Loss  0.32755690813064575\n","Batch  156  Loss  0.32884660363197327\n","Batch  157  Loss  0.33719947934150696\n","Batch  158  Loss  0.32072651386260986\n","Batch  159  Loss  0.33255571126937866\n","Batch  160  Loss  0.3215777575969696\n","Batch  161  Loss  0.3244583010673523\n","Batch  162  Loss  0.33099564909935\n","Batch  163  Loss  0.33687129616737366\n","Batch  164  Loss  0.3277221620082855\n","Batch  165  Loss  0.32611167430877686\n","Epoch: 5 , Loss: tensor(0.0538, device='cuda:0', grad_fn=<AddBackward0>)\n","Batch  1  Loss  0.3296603262424469\n","Batch  2  Loss  0.3265047073364258\n","Batch  3  Loss  0.3312968611717224\n","Batch  4  Loss  0.3208719491958618\n","Batch  5  Loss  0.3216734826564789\n","Batch  6  Loss  0.3181517720222473\n","Batch  7  Loss  0.32318589091300964\n","Batch  8  Loss  0.34160616993904114\n","Batch  9  Loss  0.3242993652820587\n","Batch  10  Loss  0.3347005546092987\n","Batch  11  Loss  0.32108739018440247\n","Batch  12  Loss  0.32448506355285645\n","Batch  13  Loss  0.3276742696762085\n","Batch  14  Loss  0.3177679777145386\n","Batch  15  Loss  0.3274003565311432\n","Batch  16  Loss  0.3110158443450928\n","Batch  17  Loss  0.32728469371795654\n","Batch  18  Loss  0.3160373568534851\n","Batch  19  Loss  0.3273172378540039\n","Batch  20  Loss  0.3140525221824646\n","Batch  21  Loss  0.3183671236038208\n","Batch  22  Loss  0.3281727731227875\n","Batch  23  Loss  0.3310823142528534\n","Batch  24  Loss  0.334044486284256\n","Batch  25  Loss  0.32209059596061707\n","Batch  26  Loss  0.3303990662097931\n","Batch  27  Loss  0.31673189997673035\n","Batch  28  Loss  0.33034729957580566\n","Batch  29  Loss  0.31535205245018005\n","Batch  30  Loss  0.3230612874031067\n","Batch  31  Loss  0.3236317038536072\n","Batch  32  Loss  0.3183249235153198\n","Batch  33  Loss  0.3340562880039215\n","Batch  34  Loss  0.325347363948822\n","Batch  35  Loss  0.31782689690589905\n","Batch  36  Loss  0.324972927570343\n","Batch  37  Loss  0.3345986008644104\n","Batch  38  Loss  0.32957613468170166\n","Batch  39  Loss  0.31281331181526184\n","Batch  40  Loss  0.3168046176433563\n","Batch  41  Loss  0.3219587206840515\n","Batch  42  Loss  0.31519174575805664\n","Batch  43  Loss  0.3237042725086212\n","Batch  44  Loss  0.3236832320690155\n","Batch  45  Loss  0.32756227254867554\n","Batch  46  Loss  0.3264986574649811\n","Batch  47  Loss  0.3266419470310211\n","Batch  48  Loss  0.31171900033950806\n","Batch  49  Loss  0.3133905231952667\n","Batch  50  Loss  0.3255871534347534\n","Batch  51  Loss  0.32198038697242737\n","Batch  52  Loss  0.330899715423584\n","Batch  53  Loss  0.3370954990386963\n","Batch  54  Loss  0.3335873782634735\n","Batch  55  Loss  0.3097796142101288\n","Batch  56  Loss  0.30962592363357544\n","Batch  57  Loss  0.3161267936229706\n","Batch  58  Loss  0.32351893186569214\n","Batch  59  Loss  0.3236505687236786\n","Batch  60  Loss  0.32108330726623535\n","Batch  61  Loss  0.3229890763759613\n","Batch  62  Loss  0.31187012791633606\n","Batch  63  Loss  0.32319146394729614\n","Batch  64  Loss  0.3206082284450531\n","Batch  65  Loss  0.3151918351650238\n","Batch  66  Loss  0.32139071822166443\n","Batch  67  Loss  0.32755112648010254\n","Batch  68  Loss  0.31786227226257324\n","Batch  69  Loss  0.33082136511802673\n","Batch  70  Loss  0.31825414299964905\n","Batch  71  Loss  0.3309733271598816\n","Batch  72  Loss  0.3210894763469696\n","Batch  73  Loss  0.32479187846183777\n","Batch  74  Loss  0.32304641604423523\n","Batch  75  Loss  0.32367315888404846\n","Batch  76  Loss  0.32618966698646545\n","Batch  77  Loss  0.31698742508888245\n","Batch  78  Loss  0.30892816185951233\n","Batch  79  Loss  0.3199741542339325\n","Batch  80  Loss  0.3105475902557373\n","Batch  81  Loss  0.33531635999679565\n","Batch  82  Loss  0.3201768696308136\n","Batch  83  Loss  0.31966811418533325\n","Batch  84  Loss  0.3158244490623474\n","Batch  85  Loss  0.3233974874019623\n","Batch  86  Loss  0.3179378807544708\n","Batch  87  Loss  0.31017929315567017\n","Batch  88  Loss  0.3254537582397461\n","Batch  89  Loss  0.3239099979400635\n","Batch  90  Loss  0.32384368777275085\n","Batch  91  Loss  0.31253018975257874\n","Batch  92  Loss  0.32253408432006836\n","Batch  93  Loss  0.31611940264701843\n","Batch  94  Loss  0.32321715354919434\n","Batch  95  Loss  0.33231258392333984\n","Batch  96  Loss  0.3254810571670532\n","Batch  97  Loss  0.3233574330806732\n","Batch  98  Loss  0.31374338269233704\n","Batch  99  Loss  0.3165057897567749\n","Batch  100  Loss  0.3104582130908966\n","Batch  101  Loss  0.31690534949302673\n","Batch  102  Loss  0.3254725933074951\n","Batch  103  Loss  0.32591530680656433\n","Batch  104  Loss  0.31154608726501465\n","Batch  105  Loss  0.32206493616104126\n","Batch  106  Loss  0.3194290101528168\n","Batch  107  Loss  0.31250056624412537\n","Batch  108  Loss  0.3282143175601959\n","Batch  109  Loss  0.32386547327041626\n","Batch  110  Loss  0.31030532717704773\n","Batch  111  Loss  0.3286176323890686\n","Batch  112  Loss  0.31901609897613525\n","Batch  113  Loss  0.323897123336792\n","Batch  114  Loss  0.3198640048503876\n","Batch  115  Loss  0.3152133524417877\n","Batch  116  Loss  0.31948480010032654\n","Batch  117  Loss  0.31656983494758606\n","Batch  118  Loss  0.3195989429950714\n","Batch  119  Loss  0.315335750579834\n","Batch  120  Loss  0.32031741738319397\n","Batch  121  Loss  0.3301124572753906\n","Batch  122  Loss  0.3279086649417877\n","Batch  123  Loss  0.32752126455307007\n","Batch  124  Loss  0.30857759714126587\n","Batch  125  Loss  0.3113747239112854\n","Batch  126  Loss  0.329511821269989\n","Batch  127  Loss  0.3127058148384094\n","Batch  128  Loss  0.30662333965301514\n","Batch  129  Loss  0.3138622045516968\n","Batch  130  Loss  0.3172110319137573\n","Batch  131  Loss  0.32729431986808777\n","Batch  132  Loss  0.3192329406738281\n","Batch  133  Loss  0.318618506193161\n","Batch  134  Loss  0.30879974365234375\n","Batch  135  Loss  0.32082095742225647\n","Batch  136  Loss  0.32467222213745117\n","Batch  137  Loss  0.3242902457714081\n","Batch  138  Loss  0.32496535778045654\n","Batch  139  Loss  0.31832852959632874\n","Batch  140  Loss  0.31389451026916504\n","Batch  141  Loss  0.32007819414138794\n","Batch  142  Loss  0.314212441444397\n","Batch  143  Loss  0.3155897259712219\n","Batch  144  Loss  0.31970250606536865\n","Batch  145  Loss  0.30419042706489563\n","Batch  146  Loss  0.32349351048469543\n","Batch  147  Loss  0.3283785581588745\n","Batch  148  Loss  0.32228755950927734\n","Batch  149  Loss  0.31223151087760925\n","Batch  150  Loss  0.3178118169307709\n","Batch  151  Loss  0.31831037998199463\n","Batch  152  Loss  0.3155798614025116\n","Batch  153  Loss  0.3221559524536133\n","Batch  154  Loss  0.3112369179725647\n","Batch  155  Loss  0.318752259016037\n","Batch  156  Loss  0.32514798641204834\n","Batch  157  Loss  0.3267229199409485\n","Batch  158  Loss  0.31627345085144043\n","Batch  159  Loss  0.3102670907974243\n","Batch  160  Loss  0.31900569796562195\n","Batch  161  Loss  0.3068639039993286\n","Batch  162  Loss  0.32406800985336304\n","Batch  163  Loss  0.3194977939128876\n","Batch  164  Loss  0.327589213848114\n","Batch  165  Loss  0.32118597626686096\n","Epoch: 6 , Loss: tensor(0.0520, device='cuda:0', grad_fn=<AddBackward0>)\n","Batch  1  Loss  0.3332515060901642\n","Batch  2  Loss  0.3244606852531433\n","Batch  3  Loss  0.31563419103622437\n","Batch  4  Loss  0.3181022107601166\n","Batch  5  Loss  0.31716325879096985\n","Batch  6  Loss  0.3098562955856323\n","Batch  7  Loss  0.32887503504753113\n","Batch  8  Loss  0.31657764315605164\n","Batch  9  Loss  0.31421637535095215\n","Batch  10  Loss  0.3282778859138489\n","Batch  11  Loss  0.32367733120918274\n","Batch  12  Loss  0.32559841871261597\n","Batch  13  Loss  0.31276947259902954\n","Batch  14  Loss  0.3136962056159973\n","Batch  15  Loss  0.3091486692428589\n","Batch  16  Loss  0.32678940892219543\n","Batch  17  Loss  0.3226437568664551\n","Batch  18  Loss  0.3121786415576935\n","Batch  19  Loss  0.3161863684654236\n","Batch  20  Loss  0.32037875056266785\n","Batch  21  Loss  0.31310707330703735\n","Batch  22  Loss  0.3214868903160095\n","Batch  23  Loss  0.31659018993377686\n","Batch  24  Loss  0.3110017776489258\n","Batch  25  Loss  0.31866320967674255\n","Batch  26  Loss  0.3241056501865387\n","Batch  27  Loss  0.322252482175827\n","Batch  28  Loss  0.3268299400806427\n","Batch  29  Loss  0.32298558950424194\n","Batch  30  Loss  0.3103095293045044\n","Batch  31  Loss  0.32822856307029724\n","Batch  32  Loss  0.31053364276885986\n","Batch  33  Loss  0.32009947299957275\n","Batch  34  Loss  0.32783445715904236\n","Batch  35  Loss  0.3159620761871338\n","Batch  36  Loss  0.32591041922569275\n","Batch  37  Loss  0.32068967819213867\n","Batch  38  Loss  0.3093109726905823\n","Batch  39  Loss  0.31264835596084595\n","Batch  40  Loss  0.32165104150772095\n","Batch  41  Loss  0.32091569900512695\n","Batch  42  Loss  0.31833773851394653\n","Batch  43  Loss  0.32445964217185974\n","Batch  44  Loss  0.31501489877700806\n","Batch  45  Loss  0.32575634121894836\n","Batch  46  Loss  0.3167027533054352\n","Batch  47  Loss  0.3190239369869232\n","Batch  48  Loss  0.30936554074287415\n","Batch  49  Loss  0.30790969729423523\n","Batch  50  Loss  0.31386473774909973\n","Batch  51  Loss  0.3134009838104248\n","Batch  52  Loss  0.32993265986442566\n","Batch  53  Loss  0.3073662519454956\n","Batch  54  Loss  0.31691938638687134\n","Batch  55  Loss  0.3186134099960327\n","Batch  56  Loss  0.31236502528190613\n","Batch  57  Loss  0.31907644867897034\n","Batch  58  Loss  0.31729868054389954\n","Batch  59  Loss  0.3079582750797272\n","Batch  60  Loss  0.31152859330177307\n","Batch  61  Loss  0.31516948342323303\n","Batch  62  Loss  0.3105902671813965\n","Batch  63  Loss  0.3160983622074127\n","Batch  64  Loss  0.3063720166683197\n","Batch  65  Loss  0.31113237142562866\n","Batch  66  Loss  0.3152635991573334\n","Batch  67  Loss  0.3085222542285919\n","Batch  68  Loss  0.31266412138938904\n","Batch  69  Loss  0.30365821719169617\n","Batch  70  Loss  0.3313688039779663\n","Batch  71  Loss  0.3157043755054474\n","Batch  72  Loss  0.3131282925605774\n","Batch  73  Loss  0.3122337758541107\n","Batch  74  Loss  0.3167778551578522\n","Batch  75  Loss  0.31923794746398926\n","Batch  76  Loss  0.312438040971756\n","Batch  77  Loss  0.31277912855148315\n","Batch  78  Loss  0.3122999370098114\n","Batch  79  Loss  0.3193757236003876\n","Batch  80  Loss  0.31513476371765137\n","Batch  81  Loss  0.3177873492240906\n","Batch  82  Loss  0.31991276144981384\n","Batch  83  Loss  0.31303995847702026\n","Batch  84  Loss  0.31316468119621277\n","Batch  85  Loss  0.3131961226463318\n","Batch  86  Loss  0.31312525272369385\n","Batch  87  Loss  0.30518293380737305\n","Batch  88  Loss  0.32833531498908997\n","Batch  89  Loss  0.3234454393386841\n","Batch  90  Loss  0.31381022930145264\n","Batch  91  Loss  0.3180425763130188\n","Batch  92  Loss  0.3221105635166168\n","Batch  93  Loss  0.3129805028438568\n","Batch  94  Loss  0.3258366286754608\n","Batch  95  Loss  0.29692134261131287\n","Batch  96  Loss  0.31419217586517334\n","Batch  97  Loss  0.32653194665908813\n","Batch  98  Loss  0.3111794888973236\n","Batch  99  Loss  0.3133058249950409\n","Batch  100  Loss  0.325338751077652\n","Batch  101  Loss  0.31561318039894104\n","Batch  102  Loss  0.31184399127960205\n","Batch  103  Loss  0.32509535551071167\n","Batch  104  Loss  0.31595516204833984\n","Batch  105  Loss  0.3181191384792328\n","Batch  106  Loss  0.3192293643951416\n","Batch  107  Loss  0.31766942143440247\n","Batch  108  Loss  0.3195534348487854\n","Batch  109  Loss  0.3131820857524872\n","Batch  110  Loss  0.3193413019180298\n","Batch  111  Loss  0.32372066378593445\n","Batch  112  Loss  0.31067773699760437\n","Batch  113  Loss  0.3142179250717163\n","Batch  114  Loss  0.32057130336761475\n","Batch  115  Loss  0.31840085983276367\n","Batch  116  Loss  0.3163433372974396\n","Batch  117  Loss  0.3215535879135132\n","Batch  118  Loss  0.3168189823627472\n","Batch  119  Loss  0.30987682938575745\n","Batch  120  Loss  0.30751729011535645\n","Batch  121  Loss  0.3294585645198822\n","Batch  122  Loss  0.30767199397087097\n","Batch  123  Loss  0.31532445549964905\n","Batch  124  Loss  0.3143521845340729\n","Batch  125  Loss  0.30739304423332214\n","Batch  126  Loss  0.30693700909614563\n","Batch  127  Loss  0.3179033100605011\n","Batch  128  Loss  0.31081920862197876\n","Batch  129  Loss  0.31435880064964294\n","Batch  130  Loss  0.31102871894836426\n","Batch  131  Loss  0.31860366463661194\n","Batch  132  Loss  0.3206562101840973\n","Batch  133  Loss  0.31353670358657837\n","Batch  134  Loss  0.31844472885131836\n","Batch  135  Loss  0.30995461344718933\n","Batch  136  Loss  0.31204739212989807\n","Batch  137  Loss  0.30780068039894104\n","Batch  138  Loss  0.30128857493400574\n","Batch  139  Loss  0.31287387013435364\n","Batch  140  Loss  0.30733057856559753\n","Batch  141  Loss  0.31596115231513977\n","Batch  142  Loss  0.31902962923049927\n","Batch  143  Loss  0.30103859305381775\n","Batch  144  Loss  0.30268391966819763\n","Batch  145  Loss  0.31656643748283386\n","Batch  146  Loss  0.31366148591041565\n","Batch  147  Loss  0.3007739782333374\n","Batch  148  Loss  0.30772626399993896\n","Batch  149  Loss  0.32589539885520935\n","Batch  150  Loss  0.3063717782497406\n","Batch  151  Loss  0.3125772774219513\n","Batch  152  Loss  0.3175666928291321\n","Batch  153  Loss  0.3105047345161438\n","Batch  154  Loss  0.307814359664917\n","Batch  155  Loss  0.3247397243976593\n","Batch  156  Loss  0.31720757484436035\n","Batch  157  Loss  0.31604522466659546\n","Batch  158  Loss  0.3166569173336029\n","Batch  159  Loss  0.3173251450061798\n","Batch  160  Loss  0.31855106353759766\n","Batch  161  Loss  0.3189515471458435\n","Batch  162  Loss  0.30736371874809265\n","Batch  163  Loss  0.31473255157470703\n","Batch  164  Loss  0.3146609663963318\n","Batch  165  Loss  0.30004584789276123\n","Epoch: 7 , Loss: tensor(0.0512, device='cuda:0', grad_fn=<AddBackward0>)\n","Batch  1  Loss  0.31915315985679626\n","Batch  2  Loss  0.3265705108642578\n","Batch  3  Loss  0.31817176938056946\n","Batch  4  Loss  0.3114607036113739\n","Batch  5  Loss  0.31643250584602356\n","Batch  6  Loss  0.3104053735733032\n","Batch  7  Loss  0.30112549662590027\n","Batch  8  Loss  0.30799493193626404\n","Batch  9  Loss  0.3258408010005951\n","Batch  10  Loss  0.3210276663303375\n","Batch  11  Loss  0.31544214487075806\n","Batch  12  Loss  0.31933099031448364\n","Batch  13  Loss  0.3206171989440918\n","Batch  14  Loss  0.3348311483860016\n","Batch  15  Loss  0.3278462588787079\n","Batch  16  Loss  0.3122085928916931\n","Batch  17  Loss  0.3212476968765259\n","Batch  18  Loss  0.3214804530143738\n","Batch  19  Loss  0.3028022050857544\n","Batch  20  Loss  0.3054584562778473\n","Batch  21  Loss  0.3346589505672455\n","Batch  22  Loss  0.3075624108314514\n","Batch  23  Loss  0.3157941699028015\n","Batch  24  Loss  0.30368906259536743\n","Batch  25  Loss  0.3169534206390381\n","Batch  26  Loss  0.30594590306282043\n","Batch  27  Loss  0.31905224919319153\n","Batch  28  Loss  0.3074621856212616\n","Batch  29  Loss  0.31215012073516846\n","Batch  30  Loss  0.32139113545417786\n","Batch  31  Loss  0.31781643629074097\n","Batch  32  Loss  0.320284903049469\n","Batch  33  Loss  0.31152862310409546\n","Batch  34  Loss  0.3116597533226013\n","Batch  35  Loss  0.31529179215431213\n","Batch  36  Loss  0.31195706129074097\n","Batch  37  Loss  0.3069731295108795\n","Batch  38  Loss  0.31582358479499817\n","Batch  39  Loss  0.3226708769798279\n","Batch  40  Loss  0.31416863203048706\n","Batch  41  Loss  0.3134613037109375\n","Batch  42  Loss  0.3128690719604492\n","Batch  43  Loss  0.3039476275444031\n","Batch  44  Loss  0.30683815479278564\n","Batch  45  Loss  0.31072062253952026\n","Batch  46  Loss  0.3139071762561798\n","Batch  47  Loss  0.30098870396614075\n","Batch  48  Loss  0.31039193272590637\n","Batch  49  Loss  0.3246404826641083\n","Batch  50  Loss  0.3008798062801361\n","Batch  51  Loss  0.31081703305244446\n","Batch  52  Loss  0.3119277358055115\n","Batch  53  Loss  0.3201998472213745\n","Batch  54  Loss  0.3133639097213745\n","Batch  55  Loss  0.3135946989059448\n","Batch  56  Loss  0.3065474033355713\n","Batch  57  Loss  0.31980079412460327\n","Batch  58  Loss  0.30722576379776\n","Batch  59  Loss  0.3042077422142029\n","Batch  60  Loss  0.31386467814445496\n","Batch  61  Loss  0.3038366138935089\n","Batch  62  Loss  0.31236523389816284\n","Batch  63  Loss  0.31555667519569397\n","Batch  64  Loss  0.3050869107246399\n","Batch  65  Loss  0.30312538146972656\n","Batch  66  Loss  0.309255987405777\n","Batch  67  Loss  0.3094751834869385\n","Batch  68  Loss  0.3012257218360901\n","Batch  69  Loss  0.3157656490802765\n","Batch  70  Loss  0.31265607476234436\n","Batch  71  Loss  0.3161989748477936\n","Batch  72  Loss  0.3065566420555115\n","Batch  73  Loss  0.30743321776390076\n","Batch  74  Loss  0.3131396770477295\n","Batch  75  Loss  0.29665669798851013\n","Batch  76  Loss  0.3010748624801636\n","Batch  77  Loss  0.3213876187801361\n","Batch  78  Loss  0.3148227334022522\n","Batch  79  Loss  0.3089846074581146\n","Batch  80  Loss  0.31391090154647827\n","Batch  81  Loss  0.31195494532585144\n","Batch  82  Loss  0.31782639026641846\n","Batch  83  Loss  0.3033756613731384\n","Batch  84  Loss  0.3099829852581024\n","Batch  85  Loss  0.30230191349983215\n","Batch  86  Loss  0.3237631022930145\n","Batch  87  Loss  0.30420517921447754\n","Batch  88  Loss  0.31104087829589844\n","Batch  89  Loss  0.3205500543117523\n","Batch  90  Loss  0.3111303150653839\n","Batch  91  Loss  0.31479665637016296\n","Batch  92  Loss  0.30475690960884094\n","Batch  93  Loss  0.30975401401519775\n","Batch  94  Loss  0.3205312192440033\n","Batch  95  Loss  0.30391237139701843\n","Batch  96  Loss  0.3061073422431946\n","Batch  97  Loss  0.31334808468818665\n","Batch  98  Loss  0.3066261410713196\n","Batch  99  Loss  0.32390108704566956\n","Batch  100  Loss  0.3166339099407196\n","Batch  101  Loss  0.3207138180732727\n","Batch  102  Loss  0.3101882338523865\n","Batch  103  Loss  0.30578410625457764\n","Batch  104  Loss  0.31339123845100403\n","Batch  105  Loss  0.3122212588787079\n","Batch  106  Loss  0.3027462661266327\n","Batch  107  Loss  0.31249287724494934\n","Batch  108  Loss  0.325350284576416\n","Batch  109  Loss  0.30646660923957825\n","Batch  110  Loss  0.3129528760910034\n","Batch  111  Loss  0.3150389492511749\n","Batch  112  Loss  0.3105621039867401\n","Batch  113  Loss  0.30460405349731445\n","Batch  114  Loss  0.3095102906227112\n","Batch  115  Loss  0.3219206929206848\n","Batch  116  Loss  0.3172213137149811\n","Batch  117  Loss  0.3030458986759186\n","Batch  118  Loss  0.31935420632362366\n","Batch  119  Loss  0.31879693269729614\n","Batch  120  Loss  0.31083011627197266\n","Batch  121  Loss  0.3028068542480469\n","Batch  122  Loss  0.3128267228603363\n","Batch  123  Loss  0.31512463092803955\n","Batch  124  Loss  0.30718836188316345\n","Batch  125  Loss  0.310310035943985\n","Batch  126  Loss  0.3098714053630829\n","Batch  127  Loss  0.3044307827949524\n","Batch  128  Loss  0.3023018538951874\n","Batch  129  Loss  0.3133562207221985\n","Batch  130  Loss  0.31689953804016113\n","Batch  131  Loss  0.31784480810165405\n","Batch  132  Loss  0.3073902726173401\n","Batch  133  Loss  0.3051290214061737\n","Batch  134  Loss  0.31962335109710693\n","Batch  135  Loss  0.3206181526184082\n","Batch  136  Loss  0.30516108870506287\n","Batch  137  Loss  0.30891790986061096\n","Batch  138  Loss  0.31268492341041565\n","Batch  139  Loss  0.31145262718200684\n","Batch  140  Loss  0.3108694553375244\n","Batch  141  Loss  0.31918245553970337\n","Batch  142  Loss  0.2992275655269623\n","Batch  143  Loss  0.30209073424339294\n","Batch  144  Loss  0.3148730397224426\n","Batch  145  Loss  0.3032396137714386\n","Batch  146  Loss  0.3112846910953522\n","Batch  147  Loss  0.3144155740737915\n","Batch  148  Loss  0.30078449845314026\n","Batch  149  Loss  0.3076569139957428\n","Batch  150  Loss  0.3196786642074585\n","Batch  151  Loss  0.2966698408126831\n","Batch  152  Loss  0.31601810455322266\n","Batch  153  Loss  0.3052048087120056\n","Batch  154  Loss  0.30216142535209656\n","Batch  155  Loss  0.3160608112812042\n","Batch  156  Loss  0.3076659142971039\n","Batch  157  Loss  0.3176409602165222\n","Batch  158  Loss  0.31080278754234314\n","Batch  159  Loss  0.31669390201568604\n","Batch  160  Loss  0.3129873275756836\n","Batch  161  Loss  0.30847474932670593\n","Batch  162  Loss  0.3131561875343323\n","Batch  163  Loss  0.31282714009284973\n","Batch  164  Loss  0.3116280436515808\n","Batch  165  Loss  0.3053297996520996\n","Epoch: 8 , Loss: tensor(0.0506, device='cuda:0', grad_fn=<AddBackward0>)\n","Batch  1  Loss  0.3097294867038727\n","Batch  2  Loss  0.3128338158130646\n","Batch  3  Loss  0.32094794511795044\n","Batch  4  Loss  0.3217460811138153\n","Batch  5  Loss  0.31692126393318176\n","Batch  6  Loss  0.30498456954956055\n","Batch  7  Loss  0.3074512481689453\n","Batch  8  Loss  0.2995701730251312\n","Batch  9  Loss  0.3122323155403137\n","Batch  10  Loss  0.3131459951400757\n","Batch  11  Loss  0.31111034750938416\n","Batch  12  Loss  0.31512022018432617\n","Batch  13  Loss  0.31354770064353943\n","Batch  14  Loss  0.3026760518550873\n","Batch  15  Loss  0.3090895116329193\n","Batch  16  Loss  0.3027905821800232\n","Batch  17  Loss  0.3074936866760254\n","Batch  18  Loss  0.30484071373939514\n","Batch  19  Loss  0.3117607831954956\n","Batch  20  Loss  0.3046643137931824\n","Batch  21  Loss  0.30116185545921326\n","Batch  22  Loss  0.3112280070781708\n","Batch  23  Loss  0.30237820744514465\n","Batch  24  Loss  0.29707953333854675\n","Batch  25  Loss  0.30594709515571594\n","Batch  26  Loss  0.31221699714660645\n","Batch  27  Loss  0.303305059671402\n","Batch  28  Loss  0.3075542747974396\n","Batch  29  Loss  0.31015729904174805\n","Batch  30  Loss  0.316170871257782\n","Batch  31  Loss  0.31320270895957947\n","Batch  32  Loss  0.3134360611438751\n","Batch  33  Loss  0.3067805767059326\n","Batch  34  Loss  0.3007606267929077\n","Batch  35  Loss  0.31085333228111267\n","Batch  36  Loss  0.30069318413734436\n","Batch  37  Loss  0.31380367279052734\n","Batch  38  Loss  0.30491289496421814\n","Batch  39  Loss  0.29306477308273315\n","Batch  40  Loss  0.31456273794174194\n","Batch  41  Loss  0.31885138154029846\n","Batch  42  Loss  0.3018328547477722\n","Batch  43  Loss  0.30585163831710815\n","Batch  44  Loss  0.3166985511779785\n","Batch  45  Loss  0.30741697549819946\n","Batch  46  Loss  0.30939817428588867\n","Batch  47  Loss  0.3123871684074402\n","Batch  48  Loss  0.3160032033920288\n","Batch  49  Loss  0.2971102297306061\n","Batch  50  Loss  0.31595927476882935\n","Batch  51  Loss  0.3148249685764313\n","Batch  52  Loss  0.3078683018684387\n","Batch  53  Loss  0.3082438111305237\n","Batch  54  Loss  0.3130944073200226\n","Batch  55  Loss  0.3171009421348572\n","Batch  56  Loss  0.3169844150543213\n","Batch  57  Loss  0.30324089527130127\n","Batch  58  Loss  0.29437047243118286\n","Batch  59  Loss  0.30943143367767334\n","Batch  60  Loss  0.30773085355758667\n","Batch  61  Loss  0.30662694573402405\n","Batch  62  Loss  0.30396562814712524\n","Batch  63  Loss  0.3098270893096924\n","Batch  64  Loss  0.3039345443248749\n","Batch  65  Loss  0.30864572525024414\n","Batch  66  Loss  0.31157153844833374\n","Batch  67  Loss  0.3155418336391449\n","Batch  68  Loss  0.313111275434494\n","Batch  69  Loss  0.2994520664215088\n","Batch  70  Loss  0.3019264042377472\n","Batch  71  Loss  0.3061663806438446\n","Batch  72  Loss  0.31130215525627136\n","Batch  73  Loss  0.30782151222229004\n","Batch  74  Loss  0.29879841208457947\n","Batch  75  Loss  0.3076058030128479\n","Batch  76  Loss  0.3021339774131775\n","Batch  77  Loss  0.2997971475124359\n","Batch  78  Loss  0.3105279505252838\n","Batch  79  Loss  0.3245038092136383\n","Batch  80  Loss  0.3048558235168457\n","Batch  81  Loss  0.3127807378768921\n","Batch  82  Loss  0.3050602078437805\n","Batch  83  Loss  0.295765221118927\n","Batch  84  Loss  0.31305062770843506\n","Batch  85  Loss  0.3072721064090729\n","Batch  86  Loss  0.31622734665870667\n","Batch  87  Loss  0.3107362985610962\n","Batch  88  Loss  0.30617913603782654\n","Batch  89  Loss  0.3069583773612976\n","Batch  90  Loss  0.3168359398841858\n","Batch  91  Loss  0.30839216709136963\n","Batch  92  Loss  0.3049777150154114\n","Batch  93  Loss  0.30305352807044983\n","Batch  94  Loss  0.31767505407333374\n","Batch  95  Loss  0.3028995990753174\n","Batch  96  Loss  0.30757060647010803\n","Batch  97  Loss  0.3094477355480194\n","Batch  98  Loss  0.31779372692108154\n","Batch  99  Loss  0.30504941940307617\n","Batch  100  Loss  0.3093019723892212\n","Batch  101  Loss  0.3067963719367981\n","Batch  102  Loss  0.30322712659835815\n","Batch  103  Loss  0.3209631145000458\n","Batch  104  Loss  0.3025878071784973\n","Batch  105  Loss  0.31654587388038635\n","Batch  106  Loss  0.31409698724746704\n","Batch  107  Loss  0.31309959292411804\n","Batch  108  Loss  0.31511688232421875\n","Batch  109  Loss  0.31019702553749084\n","Batch  110  Loss  0.31008458137512207\n","Batch  111  Loss  0.3079017400741577\n","Batch  112  Loss  0.3057699501514435\n","Batch  113  Loss  0.3036844730377197\n","Batch  114  Loss  0.3138875663280487\n","Batch  115  Loss  0.3078312277793884\n","Batch  116  Loss  0.30273035168647766\n","Batch  117  Loss  0.31183525919914246\n","Batch  118  Loss  0.3059898018836975\n","Batch  119  Loss  0.3123202323913574\n","Batch  120  Loss  0.3077363073825836\n","Batch  121  Loss  0.30261296033859253\n","Batch  122  Loss  0.30489471554756165\n","Batch  123  Loss  0.3143562376499176\n","Batch  124  Loss  0.31707334518432617\n","Batch  125  Loss  0.3109077215194702\n","Batch  126  Loss  0.3007371425628662\n","Batch  127  Loss  0.31953978538513184\n","Batch  128  Loss  0.30119970440864563\n","Batch  129  Loss  0.29792189598083496\n","Batch  130  Loss  0.3061617612838745\n","Batch  131  Loss  0.30552157759666443\n","Batch  132  Loss  0.313002347946167\n","Batch  133  Loss  0.2958418130874634\n","Batch  134  Loss  0.30472853779792786\n","Batch  135  Loss  0.31334173679351807\n","Batch  136  Loss  0.3113992512226105\n","Batch  137  Loss  0.3101431131362915\n","Batch  138  Loss  0.30911555886268616\n","Batch  139  Loss  0.29860687255859375\n","Batch  140  Loss  0.3101755380630493\n","Batch  141  Loss  0.31080710887908936\n","Batch  142  Loss  0.3082050383090973\n","Batch  143  Loss  0.31520479917526245\n","Batch  144  Loss  0.3141307830810547\n","Batch  145  Loss  0.3053653836250305\n","Batch  146  Loss  0.3109061121940613\n","Batch  147  Loss  0.2986871898174286\n","Batch  148  Loss  0.3069409728050232\n","Batch  149  Loss  0.3040515184402466\n","Batch  150  Loss  0.31037625670433044\n","Batch  151  Loss  0.3062502443790436\n","Batch  152  Loss  0.31142479181289673\n","Batch  153  Loss  0.3090757727622986\n","Batch  154  Loss  0.31226712465286255\n","Batch  155  Loss  0.31481391191482544\n","Batch  156  Loss  0.3011223375797272\n","Batch  157  Loss  0.31082138419151306\n","Batch  158  Loss  0.31483322381973267\n","Batch  159  Loss  0.3053932189941406\n","Batch  160  Loss  0.3065505027770996\n","Batch  161  Loss  0.3150361478328705\n","Batch  162  Loss  0.3100237250328064\n","Batch  163  Loss  0.3120857775211334\n","Batch  164  Loss  0.31307104229927063\n","Batch  165  Loss  0.30094942450523376\n","Epoch: 9 , Loss: tensor(0.0500, device='cuda:0', grad_fn=<AddBackward0>)\n"]}],"source":["n.train(aloader,auto_transformer, 10, loss_type = \"ZINB\",start_epoch=9)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":219,"referenced_widgets":["e4240bdb6c52412e9c9dfa627f3b01ad","c966977809b145398a53c651dd5b3cf0","f0e3c76cf540491d943c6dde5dceded4","2d2568c263224298ae8bf720c62a64ac","57d354dc7c0b4e57b245c90d7ae14097","349313de85b149a6beb69c6daa005920","d63a9230eceb4a61ac8c21f4413960aa","7d9ea1446ea34cac9654dc7d5c5e0e14","912c27a63f4e419da1510f60b09e8106","2dcbe4b801e941c8b6d9a66f645c90cc","38319ba0d9ee467e8f4bfa86d444f427"]},"executionInfo":{"elapsed":80139,"status":"ok","timestamp":1733161280643,"user":{"displayName":"Johan Lindqvist","userId":"10341636458737247069"},"user_tz":480},"id":"6lVbR0CHcFwI","outputId":"4d5afd32-b392-465d-b5c8-8347d17c737e"},"outputs":[{"name":"stderr","output_type":"stream","text":["INFO: GPU available: True (cuda), used: True\n","INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True\n","INFO: TPU available: False, using: 0 TPU cores\n","INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n","INFO: HPU available: False, using: 0 HPUs\n","INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n","INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e4240bdb6c52412e9c9dfa627f3b01ad","version_major":2,"version_minor":0},"text/plain":["Training:   0%|          | 0/10 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["INFO: `Trainer.fit` stopped: `max_epochs=10` reached.\n","INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=10` reached.\n"]}],"source":["bdata = bdata.copy()\n","scvi.model.SCVI.setup_anndata(bdata)\n","scvi = scvi.model.SCVI(bdata,n_latent = 64)\n","scvi.train(max_epochs=10)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8497,"status":"ok","timestamp":1733161539349,"user":{"displayName":"Johan Lindqvist","userId":"10341636458737247069"},"user_tz":480},"id":"94A7UsXXdple","outputId":"e7541440-d804-4c0a-c981-75d04f2eb63e"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[34mINFO    \u001b[0m Input AnnData not setup with scvi-tools. attempting to transfer AnnData setup                             \n"]}],"source":["cdata = sc.read_h5ad('drive/MyDrive/CSE527/gridchunk1_1_3.h5ad')\n","cdata = cdata[:,cdata.var.sort_values(by=\"Gene\").index]\n","cdata = cdata.copy()\n","scvi.model.SCVI.setup_anndata(cdata)\n","cdata.obsm['X_scvi'] = scvi.get_latent_representation(cdata)\n","cloader = AnnLoader(cdata, batch_size=256)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10153,"status":"ok","timestamp":1733161549493,"user":{"displayName":"Johan Lindqvist","userId":"10341636458737247069"},"user_tz":480},"id":"8-LQ6EY1drVM","outputId":"6d71467f-4d34-4025-e50b-56c7d4cbe9b2"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[34mINFO    \u001b[0m Input AnnData not setup with scvi-tools. attempting to transfer AnnData setup                             \n"]}],"source":["ddata = sc.read_h5ad('drive/MyDrive/CSE527/gridchunk1_1_1.h5ad')\n","ddata = ddata[:,ddata.var.sort_values(by=\"Gene\").index]\n","ddata = ddata.copy()\n","scvi.model.SCVI.setup_anndata(ddata)\n","ddata.obsm['X_scvi'] = scvi.get_latent_representation(ddata)\n","dloader = AnnLoader(cdata, batch_size=256)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DLGITXsjK_5l"},"outputs":[],"source":["model_parameters = filter(lambda p: p.requires_grad, auto_transformer.parameters())\n","params = sum([np.prod(p.size()) for p in model_parameters])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":464,"status":"ok","timestamp":1733160698467,"user":{"displayName":"Johan Lindqvist","userId":"10341636458737247069"},"user_tz":480},"id":"SkxobgUwbWrN","outputId":"1e1cb2d4-90e3-45f1-b754-b6024b180a89"},"outputs":[{"data":{"text/plain":["123522652"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["params"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4t26vpHcvnJ2"},"outputs":[],"source":["enc_lin = [512,256,64]\n","dec_lin = [512,256,64]\n","soi = [[40,1000,1487,4],[40,500,1000,4],[40,100,500,4],[40,20,100,4]]\n","auto_transformer = m.TransformerAutoencoder(input_size=59480, soih=soi,enc_lin = enc_lin, dec_lin = dec_lin, lr = 0.003, loss=\"ZINB\")\n","auto_transformer = auto_transformer.to('cuda')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3816,"status":"ok","timestamp":1733011774444,"user":{"displayName":"Johan Lindqvist","userId":"10341636458737247069"},"user_tz":480},"id":"o6yuYVo3dyIS","outputId":"f53642d0-152b-44c0-f8e1-20830d210122"},"outputs":[{"data":{"text/plain":["AutoEncoder(\n","  (encoder): Sequential(\n","    (0): Linear(in_features=59480, out_features=2048, bias=True)\n","    (1): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): LeakyReLU(negative_slope=0.01)\n","    (3): Dropout(p=0.4, inplace=False)\n","    (4): Linear(in_features=2048, out_features=64, bias=True)\n","    (5): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (6): LeakyReLU(negative_slope=0.01)\n","  )\n","  (decoder): Sequential(\n","    (0): Linear(in_features=64, out_features=2048, bias=True)\n","    (1): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): LeakyReLU(negative_slope=0.01)\n","    (3): Linear(in_features=2048, out_features=59480, bias=True)\n","    (4): BatchNorm1d(59480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (5): ReLU()\n","  )\n",")"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["auto_basic = m.AutoEncoder([0]*59480)\n","auto_basic.to('cuda')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GxtE3T4b0FyN"},"outputs":[],"source":["\n","encoder = LabelEncoder()\n","encoder.fit(bdata.obs[\"supercluster_term\"])\n","classifier = m.BasicNeuralNetwork(64*[0], bdata.obs[\"supercluster_term\"],encoder,[256,256], lr = 0.005)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3LNGxnh9etWr"},"outputs":[],"source":["classifier2 = m.BasicNeuralNetwork(64*[0], bdata.obs[\"supercluster_term\"],encoder,[256,256], lr = 0.005)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"duPIEoR6V4Zy"},"outputs":[],"source":["classifier2= classifier2.to('cuda')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3907,"status":"ok","timestamp":1733161618048,"user":{"displayName":"Johan Lindqvist","userId":"10341636458737247069"},"user_tz":480},"id":"F-l5OV1ze061","outputId":"f5bffb8d-b1ff-471d-87a0-a65caa344a74"},"outputs":[{"name":"stdout","output_type":"stream","text":["Batch  1  Loss  3.154768466949463\n","Batch  2  Loss  2.361210346221924\n","Batch  3  Loss  1.8765997886657715\n","Batch  4  Loss  1.755313515663147\n","Batch  5  Loss  1.5995681285858154\n","Batch  6  Loss  1.4947258234024048\n","Batch  7  Loss  1.497475504875183\n","Batch  8  Loss  1.454055666923523\n","Batch  9  Loss  1.2367526292800903\n","Batch  10  Loss  1.1735377311706543\n","Batch  11  Loss  1.0381810665130615\n","Batch  12  Loss  1.1280617713928223\n","Batch  13  Loss  1.0826324224472046\n","Batch  14  Loss  0.8808279633522034\n","Batch  15  Loss  0.7703836560249329\n","Batch  16  Loss  0.7882378101348877\n","Batch  17  Loss  0.8499500751495361\n","Batch  18  Loss  0.9063591957092285\n","Batch  19  Loss  0.7188360691070557\n","Batch  20  Loss  0.6936371326446533\n","Batch  21  Loss  0.7297769784927368\n","Batch  22  Loss  0.7734454870223999\n","Batch  23  Loss  0.7297844886779785\n","Batch  24  Loss  0.6971701979637146\n","Batch  25  Loss  0.7129335999488831\n","Batch  26  Loss  0.7393630146980286\n","Batch  27  Loss  0.7111684083938599\n","Batch  28  Loss  0.6828168630599976\n","Batch  29  Loss  0.5878609418869019\n","Batch  30  Loss  0.7127137780189514\n","Batch  31  Loss  0.7314435839653015\n","Batch  32  Loss  0.7774272561073303\n","Batch  33  Loss  0.6623638272285461\n","Batch  34  Loss  0.5580371022224426\n","Batch  35  Loss  0.6949343085289001\n","Batch  36  Loss  0.6778507828712463\n","Batch  37  Loss  0.6238783597946167\n","Batch  38  Loss  0.5887008905410767\n","Batch  39  Loss  0.624994158744812\n","Batch  40  Loss  0.5119380950927734\n","Batch  41  Loss  0.6885945200920105\n","Batch  42  Loss  0.6691671013832092\n","Epoch: 0 , Loss: tensor(0.1798, device='cuda:0', grad_fn=<AddBackward0>)\n","Batch  1  Loss  0.6578792929649353\n","Batch  2  Loss  0.6470887064933777\n","Batch  3  Loss  0.580761194229126\n","Batch  4  Loss  0.6848862767219543\n","Batch  5  Loss  0.6963759660720825\n","Batch  6  Loss  0.6252660155296326\n","Batch  7  Loss  0.689747154712677\n","Batch  8  Loss  0.6950446963310242\n","Batch  9  Loss  0.5790483951568604\n","Batch  10  Loss  0.5652599930763245\n","Batch  11  Loss  0.5792630910873413\n","Batch  12  Loss  0.6732302308082581\n","Batch  13  Loss  0.6288501620292664\n","Batch  14  Loss  0.459857702255249\n","Batch  15  Loss  0.451134592294693\n","Batch  16  Loss  0.42116305232048035\n","Batch  17  Loss  0.5425370335578918\n","Batch  18  Loss  0.5584338903427124\n","Batch  19  Loss  0.4097907841205597\n","Batch  20  Loss  0.38890042901039124\n","Batch  21  Loss  0.5207369923591614\n","Batch  22  Loss  0.5424070358276367\n","Batch  23  Loss  0.563726007938385\n","Batch  24  Loss  0.4545300006866455\n","Batch  25  Loss  0.43663638830184937\n","Batch  26  Loss  0.5519251823425293\n","Batch  27  Loss  0.4422985017299652\n","Batch  28  Loss  0.4816252291202545\n","Batch  29  Loss  0.48467761278152466\n","Batch  30  Loss  0.5122001767158508\n","Batch  31  Loss  0.538330614566803\n","Batch  32  Loss  0.5344657301902771\n","Batch  33  Loss  0.4652768075466156\n","Batch  34  Loss  0.38265976309776306\n","Batch  35  Loss  0.5013523101806641\n","Batch  36  Loss  0.48543867468833923\n","Batch  37  Loss  0.48710522055625916\n","Batch  38  Loss  0.44790560007095337\n","Batch  39  Loss  0.47840145230293274\n","Batch  40  Loss  0.35418230295181274\n","Batch  41  Loss  0.5522975325584412\n","Batch  42  Loss  0.49980637431144714\n","Epoch: 1 , Loss: tensor(0.1006, device='cuda:0', grad_fn=<AddBackward0>)\n","Batch  1  Loss  0.5374388694763184\n","Batch  2  Loss  0.47948014736175537\n","Batch  3  Loss  0.39355331659317017\n","Batch  4  Loss  0.5284008979797363\n","Batch  5  Loss  0.5516139268875122\n","Batch  6  Loss  0.4918995797634125\n","Batch  7  Loss  0.5785812139511108\n","Batch  8  Loss  0.5990728139877319\n","Batch  9  Loss  0.4239848852157593\n","Batch  10  Loss  0.4901655316352844\n","Batch  11  Loss  0.4497114419937134\n","Batch  12  Loss  0.5531893968582153\n","Batch  13  Loss  0.5024778246879578\n","Batch  14  Loss  0.3908751904964447\n","Batch  15  Loss  0.37368258833885193\n","Batch  16  Loss  0.3546696603298187\n","Batch  17  Loss  0.49090418219566345\n","Batch  18  Loss  0.43779993057250977\n","Batch  19  Loss  0.37462684512138367\n","Batch  20  Loss  0.3253837823867798\n","Batch  21  Loss  0.4863603711128235\n","Batch  22  Loss  0.4859163761138916\n","Batch  23  Loss  0.4670875072479248\n","Batch  24  Loss  0.4015529155731201\n","Batch  25  Loss  0.42209291458129883\n","Batch  26  Loss  0.4639876186847687\n","Batch  27  Loss  0.3889148533344269\n","Batch  28  Loss  0.3944447636604309\n","Batch  29  Loss  0.39645183086395264\n","Batch  30  Loss  0.44542714953422546\n","Batch  31  Loss  0.44021764397621155\n","Batch  32  Loss  0.5534915328025818\n","Batch  33  Loss  0.3882142901420593\n","Batch  34  Loss  0.3521328866481781\n","Batch  35  Loss  0.443011999130249\n","Batch  36  Loss  0.4454801678657532\n","Batch  37  Loss  0.452303409576416\n","Batch  38  Loss  0.3974735736846924\n","Batch  39  Loss  0.4190288782119751\n","Batch  40  Loss  0.31427866220474243\n","Batch  41  Loss  0.4613478183746338\n","Batch  42  Loss  0.507253110408783\n","Epoch: 2 , Loss: tensor(0.0875, device='cuda:0', grad_fn=<AddBackward0>)\n","Batch  1  Loss  0.47744572162628174\n","Batch  2  Loss  0.4137513339519501\n","Batch  3  Loss  0.38817155361175537\n","Batch  4  Loss  0.49412307143211365\n","Batch  5  Loss  0.5039816498756409\n","Batch  6  Loss  0.4212910830974579\n","Batch  7  Loss  0.5262741446495056\n","Batch  8  Loss  0.49831271171569824\n","Batch  9  Loss  0.428455114364624\n","Batch  10  Loss  0.4547584652900696\n","Batch  11  Loss  0.42057886719703674\n","Batch  12  Loss  0.5379325747489929\n","Batch  13  Loss  0.461666077375412\n","Batch  14  Loss  0.35068973898887634\n","Batch  15  Loss  0.39676573872566223\n","Batch  16  Loss  0.30402055382728577\n","Batch  17  Loss  0.4636523425579071\n","Batch  18  Loss  0.40103837847709656\n","Batch  19  Loss  0.3382014036178589\n","Batch  20  Loss  0.3160231411457062\n","Batch  21  Loss  0.4666658341884613\n","Batch  22  Loss  0.47311392426490784\n","Batch  23  Loss  0.3929200768470764\n","Batch  24  Loss  0.3905218243598938\n","Batch  25  Loss  0.35265490412712097\n","Batch  26  Loss  0.43888333439826965\n","Batch  27  Loss  0.3887247145175934\n","Batch  28  Loss  0.3987668454647064\n","Batch  29  Loss  0.34615492820739746\n","Batch  30  Loss  0.40796610713005066\n","Batch  31  Loss  0.4487156271934509\n","Batch  32  Loss  0.4490010738372803\n","Batch  33  Loss  0.3983219861984253\n","Batch  34  Loss  0.31481775641441345\n","Batch  35  Loss  0.38324737548828125\n","Batch  36  Loss  0.4050179719924927\n","Batch  37  Loss  0.3888435363769531\n","Batch  38  Loss  0.3378724455833435\n","Batch  39  Loss  0.4396611154079437\n","Batch  40  Loss  0.2561487555503845\n","Batch  41  Loss  0.39915648102760315\n","Batch  42  Loss  0.39584919810295105\n","Epoch: 3 , Loss: tensor(0.0783, device='cuda:0', grad_fn=<AddBackward0>)\n","Batch  1  Loss  0.4265914261341095\n","Batch  2  Loss  0.35834598541259766\n","Batch  3  Loss  0.37435510754585266\n","Batch  4  Loss  0.4048480689525604\n","Batch  5  Loss  0.4747439920902252\n","Batch  6  Loss  0.40041348338127136\n","Batch  7  Loss  0.4719596207141876\n","Batch  8  Loss  0.47275510430336\n","Batch  9  Loss  0.34658190608024597\n","Batch  10  Loss  0.4244462549686432\n","Batch  11  Loss  0.3759220242500305\n","Batch  12  Loss  0.45761027932167053\n","Batch  13  Loss  0.3980785012245178\n","Batch  14  Loss  0.31105905771255493\n","Batch  15  Loss  0.33469104766845703\n","Batch  16  Loss  0.30650594830513\n","Batch  17  Loss  0.40081390738487244\n","Batch  18  Loss  0.40968599915504456\n","Batch  19  Loss  0.30681732296943665\n","Batch  20  Loss  0.27497273683547974\n","Batch  21  Loss  0.40541473031044006\n","Batch  22  Loss  0.4300812780857086\n","Batch  23  Loss  0.4000220000743866\n","Batch  24  Loss  0.3609117269515991\n","Batch  25  Loss  0.3533456027507782\n","Batch  26  Loss  0.4259917140007019\n","Batch  27  Loss  0.3090170919895172\n","Batch  28  Loss  0.3495705723762512\n","Batch  29  Loss  0.32669126987457275\n","Batch  30  Loss  0.3870050013065338\n","Batch  31  Loss  0.4057111144065857\n","Batch  32  Loss  0.46749886870384216\n","Batch  33  Loss  0.373223215341568\n","Batch  34  Loss  0.3173755407333374\n","Batch  35  Loss  0.4289356768131256\n","Batch  36  Loss  0.36137038469314575\n","Batch  37  Loss  0.34824109077453613\n","Batch  38  Loss  0.3809536397457123\n","Batch  39  Loss  0.39709144830703735\n","Batch  40  Loss  0.25527051091194153\n","Batch  41  Loss  0.40806296467781067\n","Batch  42  Loss  0.30768322944641113\n","Epoch: 4 , Loss: tensor(0.0706, device='cuda:0', grad_fn=<AddBackward0>)\n","Batch  1  Loss  0.3889213800430298\n","Batch  2  Loss  0.34612181782722473\n","Batch  3  Loss  0.3189626634120941\n","Batch  4  Loss  0.4088623523712158\n","Batch  5  Loss  0.5175668597221375\n","Batch  6  Loss  0.4021294414997101\n","Batch  7  Loss  0.4262605905532837\n","Batch  8  Loss  0.3961222469806671\n","Batch  9  Loss  0.34849172830581665\n","Batch  10  Loss  0.40330255031585693\n","Batch  11  Loss  0.36984357237815857\n","Batch  12  Loss  0.40388745069503784\n","Batch  13  Loss  0.3706502914428711\n","Batch  14  Loss  0.3188033998012543\n","Batch  15  Loss  0.33945780992507935\n","Batch  16  Loss  0.2706104516983032\n","Batch  17  Loss  0.3991777002811432\n","Batch  18  Loss  0.35074856877326965\n","Batch  19  Loss  0.2963976562023163\n","Batch  20  Loss  0.26318755745887756\n","Batch  21  Loss  0.36147063970565796\n","Batch  22  Loss  0.38021528720855713\n","Batch  23  Loss  0.39410123229026794\n","Batch  24  Loss  0.33744505047798157\n","Batch  25  Loss  0.33368590474128723\n","Batch  26  Loss  0.3997967839241028\n","Batch  27  Loss  0.3054259121417999\n","Batch  28  Loss  0.34177133440971375\n","Batch  29  Loss  0.33772411942481995\n","Batch  30  Loss  0.33522269129753113\n","Batch  31  Loss  0.38250985741615295\n","Batch  32  Loss  0.3997121751308441\n","Batch  33  Loss  0.3329531252384186\n","Batch  34  Loss  0.3040700852870941\n","Batch  35  Loss  0.35861244797706604\n","Batch  36  Loss  0.3900250792503357\n","Batch  37  Loss  0.37029752135276794\n","Batch  38  Loss  0.3426055610179901\n","Batch  39  Loss  0.3553504943847656\n","Batch  40  Loss  0.23819918930530548\n","Batch  41  Loss  0.3360860347747803\n","Batch  42  Loss  0.3075440526008606\n","Epoch: 5 , Loss: tensor(0.0669, device='cuda:0', grad_fn=<AddBackward0>)\n","Batch  1  Loss  0.35916203260421753\n","Batch  2  Loss  0.3387286365032196\n","Batch  3  Loss  0.34550902247428894\n","Batch  4  Loss  0.42754897475242615\n","Batch  5  Loss  0.43496134877204895\n","Batch  6  Loss  0.3421057462692261\n","Batch  7  Loss  0.3887809216976166\n","Batch  8  Loss  0.412966787815094\n","Batch  9  Loss  0.3249770700931549\n","Batch  10  Loss  0.3815009593963623\n","Batch  11  Loss  0.3384806513786316\n","Batch  12  Loss  0.40355184674263\n","Batch  13  Loss  0.3989051580429077\n","Batch  14  Loss  0.2889972925186157\n","Batch  15  Loss  0.3461759388446808\n","Batch  16  Loss  0.2684069573879242\n","Batch  17  Loss  0.3910702168941498\n","Batch  18  Loss  0.34962815046310425\n","Batch  19  Loss  0.2566682994365692\n","Batch  20  Loss  0.26152554154396057\n","Batch  21  Loss  0.3665141761302948\n","Batch  22  Loss  0.37248221039772034\n","Batch  23  Loss  0.36329084634780884\n","Batch  24  Loss  0.2817482054233551\n","Batch  25  Loss  0.3190808594226837\n","Batch  26  Loss  0.3744010925292969\n","Batch  27  Loss  0.26542288064956665\n","Batch  28  Loss  0.32658839225769043\n","Batch  29  Loss  0.307876318693161\n","Batch  30  Loss  0.29371345043182373\n","Batch  31  Loss  0.38206925988197327\n","Batch  32  Loss  0.41270893812179565\n","Batch  33  Loss  0.29893770813941956\n","Batch  34  Loss  0.2699393630027771\n","Batch  35  Loss  0.3655535876750946\n","Batch  36  Loss  0.3482435941696167\n","Batch  37  Loss  0.3516160845756531\n","Batch  38  Loss  0.34506916999816895\n","Batch  39  Loss  0.37717708945274353\n","Batch  40  Loss  0.21917051076889038\n","Batch  41  Loss  0.33842387795448303\n","Batch  42  Loss  0.29425370693206787\n","Epoch: 6 , Loss: tensor(0.0640, device='cuda:0', grad_fn=<AddBackward0>)\n","Batch  1  Loss  0.3516595661640167\n","Batch  2  Loss  0.32349222898483276\n","Batch  3  Loss  0.3418497145175934\n","Batch  4  Loss  0.35691002011299133\n","Batch  5  Loss  0.4259887933731079\n","Batch  6  Loss  0.346003919839859\n","Batch  7  Loss  0.37584319710731506\n","Batch  8  Loss  0.4511842727661133\n","Batch  9  Loss  0.3112865090370178\n","Batch  10  Loss  0.37447109818458557\n","Batch  11  Loss  0.3317168951034546\n","Batch  12  Loss  0.4547228217124939\n","Batch  13  Loss  0.3905569016933441\n","Batch  14  Loss  0.28351476788520813\n","Batch  15  Loss  0.33432868123054504\n","Batch  16  Loss  0.2284964621067047\n","Batch  17  Loss  0.4028465747833252\n","Batch  18  Loss  0.34879612922668457\n","Batch  19  Loss  0.22451284527778625\n","Batch  20  Loss  0.2426632046699524\n","Batch  21  Loss  0.3905973434448242\n","Batch  22  Loss  0.3396541178226471\n","Batch  23  Loss  0.40009790658950806\n","Batch  24  Loss  0.31518518924713135\n","Batch  25  Loss  0.3076733648777008\n","Batch  26  Loss  0.3414813280105591\n","Batch  27  Loss  0.32260578870773315\n","Batch  28  Loss  0.3325883746147156\n","Batch  29  Loss  0.3308115601539612\n","Batch  30  Loss  0.3409713804721832\n","Batch  31  Loss  0.3761071562767029\n","Batch  32  Loss  0.39693400263786316\n","Batch  33  Loss  0.3324079215526581\n","Batch  34  Loss  0.27719542384147644\n","Batch  35  Loss  0.3519013226032257\n","Batch  36  Loss  0.3345843255519867\n","Batch  37  Loss  0.32804012298583984\n","Batch  38  Loss  0.30171602964401245\n","Batch  39  Loss  0.352516770362854\n","Batch  40  Loss  0.24175432324409485\n","Batch  41  Loss  0.32930073142051697\n","Batch  42  Loss  0.26971402764320374\n","Epoch: 7 , Loss: tensor(0.0629, device='cuda:0', grad_fn=<AddBackward0>)\n","Batch  1  Loss  0.3411436378955841\n","Batch  2  Loss  0.3232041895389557\n","Batch  3  Loss  0.29297009110450745\n","Batch  4  Loss  0.3836248219013214\n","Batch  5  Loss  0.44363564252853394\n","Batch  6  Loss  0.33415117859840393\n","Batch  7  Loss  0.3772803544998169\n","Batch  8  Loss  0.3441493809223175\n","Batch  9  Loss  0.301724910736084\n","Batch  10  Loss  0.35377341508865356\n","Batch  11  Loss  0.31152430176734924\n","Batch  12  Loss  0.3459084630012512\n","Batch  13  Loss  0.3499918580055237\n","Batch  14  Loss  0.2773045599460602\n","Batch  15  Loss  0.3190383315086365\n","Batch  16  Loss  0.25128787755966187\n","Batch  17  Loss  0.3293745815753937\n","Batch  18  Loss  0.33243033289909363\n","Batch  19  Loss  0.2513323128223419\n","Batch  20  Loss  0.23254479467868805\n","Batch  21  Loss  0.34752222895622253\n","Batch  22  Loss  0.3915374279022217\n","Batch  23  Loss  0.33854737877845764\n","Batch  24  Loss  0.27195024490356445\n","Batch  25  Loss  0.3066420257091522\n","Batch  26  Loss  0.352178692817688\n","Batch  27  Loss  0.2915235757827759\n","Batch  28  Loss  0.2867799401283264\n","Batch  29  Loss  0.29112428426742554\n","Batch  30  Loss  0.3389911651611328\n","Batch  31  Loss  0.34766799211502075\n","Batch  32  Loss  0.37610989809036255\n","Batch  33  Loss  0.2925885319709778\n","Batch  34  Loss  0.28510522842407227\n","Batch  35  Loss  0.3057336211204529\n","Batch  36  Loss  0.29644128680229187\n","Batch  37  Loss  0.2799294590950012\n","Batch  38  Loss  0.2596374452114105\n","Batch  39  Loss  0.33169394731521606\n","Batch  40  Loss  0.24484243988990784\n","Batch  41  Loss  0.3425447940826416\n","Batch  42  Loss  0.3313952684402466\n","Epoch: 8 , Loss: tensor(0.0614, device='cuda:0', grad_fn=<AddBackward0>)\n","Batch  1  Loss  0.33678433299064636\n","Batch  2  Loss  0.31002843379974365\n","Batch  3  Loss  0.28739020228385925\n","Batch  4  Loss  0.37227877974510193\n","Batch  5  Loss  0.4048529267311096\n","Batch  6  Loss  0.32167813181877136\n","Batch  7  Loss  0.36139142513275146\n","Batch  8  Loss  0.3415180742740631\n","Batch  9  Loss  0.3232196867465973\n","Batch  10  Loss  0.33834099769592285\n","Batch  11  Loss  0.33453863859176636\n","Batch  12  Loss  0.4122334420681\n","Batch  13  Loss  0.3160999119281769\n","Batch  14  Loss  0.24449492990970612\n","Batch  15  Loss  0.29311150312423706\n","Batch  16  Loss  0.24779389798641205\n","Batch  17  Loss  0.32110515236854553\n","Batch  18  Loss  0.330687552690506\n","Batch  19  Loss  0.24254965782165527\n","Batch  20  Loss  0.23112669587135315\n","Batch  21  Loss  0.35838016867637634\n","Batch  22  Loss  0.33175602555274963\n","Batch  23  Loss  0.31773924827575684\n","Batch  24  Loss  0.2922133505344391\n","Batch  25  Loss  0.2743556797504425\n","Batch  26  Loss  0.35114121437072754\n","Batch  27  Loss  0.2687894105911255\n","Batch  28  Loss  0.27461811900138855\n","Batch  29  Loss  0.2808399200439453\n","Batch  30  Loss  0.3190110921859741\n","Batch  31  Loss  0.3826550841331482\n","Batch  32  Loss  0.3412862718105316\n","Batch  33  Loss  0.2943325340747833\n","Batch  34  Loss  0.24471120536327362\n","Batch  35  Loss  0.32526206970214844\n","Batch  36  Loss  0.3025543689727783\n","Batch  37  Loss  0.28067779541015625\n","Batch  38  Loss  0.2957841753959656\n","Batch  39  Loss  0.3383132517337799\n","Batch  40  Loss  0.2220717966556549\n","Batch  41  Loss  0.31718552112579346\n","Batch  42  Loss  0.19346833229064941\n","Epoch: 9 , Loss: tensor(0.0560, device='cuda:0', grad_fn=<AddBackward0>)\n"]}],"source":["m.train(cloader,classifier, 10, loss_type = \"cell_type\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":275917,"status":"ok","timestamp":1733161066839,"user":{"displayName":"Johan Lindqvist","userId":"10341636458737247069"},"user_tz":480},"id":"XiEo6HhuSIrP","outputId":"3d9ac745-ffb2-42b3-ce05-aeee473c0204"},"outputs":[{"name":"stdout","output_type":"stream","text":["Batch  1  Loss  10.171245574951172\n","Batch  2  Loss  8.921320915222168\n","Batch  3  Loss  7.898580551147461\n","Batch  4  Loss  6.782512664794922\n","Batch  5  Loss  5.777307510375977\n","Batch  6  Loss  5.289464950561523\n","Batch  7  Loss  5.112739562988281\n","Batch  8  Loss  4.867040634155273\n","Batch  9  Loss  4.581319332122803\n","Batch  10  Loss  4.442652225494385\n","Batch  11  Loss  4.411296367645264\n","Batch  12  Loss  3.909832000732422\n","Batch  13  Loss  3.630112886428833\n","Batch  14  Loss  3.8357112407684326\n","Batch  15  Loss  3.5742313861846924\n","Batch  16  Loss  3.5355446338653564\n","Batch  17  Loss  3.6334190368652344\n","Batch  18  Loss  3.229795217514038\n","Batch  19  Loss  3.565889358520508\n","Batch  20  Loss  3.3743605613708496\n","Batch  21  Loss  3.054731607437134\n","Batch  22  Loss  3.2558841705322266\n","Batch  23  Loss  3.0915355682373047\n","Batch  24  Loss  3.284111738204956\n","Batch  25  Loss  3.107218027114868\n","Batch  26  Loss  3.2896625995635986\n","Batch  27  Loss  3.0534558296203613\n","Batch  28  Loss  3.015115737915039\n","Batch  29  Loss  2.9963972568511963\n","Batch  30  Loss  3.033090114593506\n","Batch  31  Loss  3.0440473556518555\n","Batch  32  Loss  3.0599045753479004\n","Batch  33  Loss  2.8891921043395996\n","Batch  34  Loss  3.031392812728882\n","Batch  35  Loss  3.205125093460083\n","Batch  36  Loss  2.9604573249816895\n","Batch  37  Loss  3.062007427215576\n","Batch  38  Loss  3.036808490753174\n","Batch  39  Loss  3.1228246688842773\n","Batch  40  Loss  3.0418169498443604\n","Batch  41  Loss  2.9270570278167725\n","Batch  42  Loss  2.96038556098938\n","Epoch: 0 , Loss: tensor(0.7375, device='cuda:0', grad_fn=<AddBackward0>)\n","Batch  1  Loss  3.078171730041504\n","Batch  2  Loss  3.0408968925476074\n","Batch  3  Loss  2.8488402366638184\n","Batch  4  Loss  2.914529323577881\n","Batch  5  Loss  2.9546871185302734\n","Batch  6  Loss  3.2009871006011963\n","Batch  7  Loss  2.9745748043060303\n","Batch  8  Loss  2.9205899238586426\n","Batch  9  Loss  2.982825517654419\n","Batch  10  Loss  3.0548617839813232\n","Batch  11  Loss  2.880262851715088\n","Batch  12  Loss  2.8745672702789307\n","Batch  13  Loss  2.7833471298217773\n","Batch  14  Loss  3.0089004039764404\n","Batch  15  Loss  2.965911865234375\n","Batch  16  Loss  2.8356378078460693\n","Batch  17  Loss  2.936893939971924\n","Batch  18  Loss  2.869699716567993\n","Batch  19  Loss  2.888556718826294\n","Batch  20  Loss  2.9183266162872314\n","Batch  21  Loss  2.956641912460327\n","Batch  22  Loss  2.812272310256958\n","Batch  23  Loss  2.8668885231018066\n","Batch  24  Loss  2.91135311126709\n","Batch  25  Loss  2.8023457527160645\n","Batch  26  Loss  2.916421890258789\n","Batch  27  Loss  2.791111707687378\n","Batch  28  Loss  2.9023075103759766\n","Batch  29  Loss  2.8789379596710205\n","Batch  30  Loss  2.8858938217163086\n","Batch  31  Loss  2.994586944580078\n","Batch  32  Loss  2.9305331707000732\n","Batch  33  Loss  2.8620452880859375\n","Batch  34  Loss  2.9431910514831543\n","Batch  35  Loss  2.857872486114502\n","Batch  36  Loss  2.8888800144195557\n","Batch  37  Loss  2.8532912731170654\n","Batch  38  Loss  2.9130570888519287\n","Batch  39  Loss  2.8229098320007324\n","Batch  40  Loss  2.8212838172912598\n","Batch  41  Loss  2.731447458267212\n","Batch  42  Loss  3.2819035053253174\n","Epoch: 1 , Loss: tensor(0.5685, device='cuda:0', grad_fn=<AddBackward0>)\n","Batch  1  Loss  2.9124131202697754\n","Batch  2  Loss  2.8776156902313232\n","Batch  3  Loss  2.8275668621063232\n","Batch  4  Loss  2.903351306915283\n","Batch  5  Loss  3.0521240234375\n","Batch  6  Loss  2.8731021881103516\n","Batch  7  Loss  3.073263645172119\n","Batch  8  Loss  2.8972837924957275\n","Batch  9  Loss  2.8266570568084717\n","Batch  10  Loss  2.839725971221924\n","Batch  11  Loss  2.940838575363159\n","Batch  12  Loss  2.629906415939331\n","Batch  13  Loss  2.937962293624878\n","Batch  14  Loss  2.7553930282592773\n","Batch  15  Loss  2.8760337829589844\n","Batch  16  Loss  2.896773099899292\n","Batch  17  Loss  2.7310118675231934\n","Batch  18  Loss  2.8703367710113525\n","Batch  19  Loss  2.860746145248413\n","Batch  20  Loss  2.7399203777313232\n","Batch  21  Loss  2.8253071308135986\n","Batch  22  Loss  2.7921535968780518\n","Batch  23  Loss  2.743429660797119\n","Batch  24  Loss  2.867677927017212\n","Batch  25  Loss  2.743626594543457\n","Batch  26  Loss  2.8591368198394775\n","Batch  27  Loss  2.908118486404419\n","Batch  28  Loss  2.8922808170318604\n","Batch  29  Loss  2.8676345348358154\n","Batch  30  Loss  2.8186373710632324\n","Batch  31  Loss  2.857731819152832\n","Batch  32  Loss  2.8480260372161865\n","Batch  33  Loss  2.8270764350891113\n","Batch  34  Loss  2.9107186794281006\n","Batch  35  Loss  2.771185874938965\n","Batch  36  Loss  2.7779622077941895\n","Batch  37  Loss  2.9739084243774414\n","Batch  38  Loss  2.7763071060180664\n","Batch  39  Loss  2.9366753101348877\n","Batch  40  Loss  2.8243000507354736\n","Batch  41  Loss  2.8609306812286377\n","Batch  42  Loss  2.701117515563965\n","Epoch: 2 , Loss: tensor(0.5415, device='cuda:0', grad_fn=<AddBackward0>)\n","Batch  1  Loss  2.8886470794677734\n","Batch  2  Loss  2.908005714416504\n","Batch  3  Loss  2.8825290203094482\n","Batch  4  Loss  2.8031222820281982\n","Batch  5  Loss  2.823133945465088\n","Batch  6  Loss  2.9098987579345703\n","Batch  7  Loss  2.7359461784362793\n","Batch  8  Loss  2.977437734603882\n","Batch  9  Loss  2.846916437149048\n","Batch  10  Loss  2.8014285564422607\n","Batch  11  Loss  2.708338975906372\n","Batch  12  Loss  2.8766496181488037\n","Batch  13  Loss  2.842858076095581\n","Batch  14  Loss  2.887906312942505\n","Batch  15  Loss  2.7306067943573\n","Batch  16  Loss  2.9542248249053955\n","Batch  17  Loss  2.737396717071533\n","Batch  18  Loss  2.884650468826294\n","Batch  19  Loss  2.8501956462860107\n","Batch  20  Loss  2.8451168537139893\n","Batch  21  Loss  2.8499867916107178\n","Batch  22  Loss  2.8908376693725586\n","Batch  23  Loss  2.882368803024292\n","Batch  24  Loss  2.887240409851074\n","Batch  25  Loss  2.757902145385742\n","Batch  26  Loss  2.860490560531616\n","Batch  27  Loss  2.841012716293335\n","Batch  28  Loss  2.7662458419799805\n","Batch  29  Loss  2.8286373615264893\n","Batch  30  Loss  2.774264097213745\n","Batch  31  Loss  2.833073616027832\n","Batch  32  Loss  2.858633518218994\n","Batch  33  Loss  2.7828893661499023\n","Batch  34  Loss  2.865081310272217\n","Batch  35  Loss  2.8305423259735107\n","Batch  36  Loss  2.8057680130004883\n","Batch  37  Loss  2.8254361152648926\n","Batch  38  Loss  2.73982834815979\n","Batch  39  Loss  2.763118267059326\n","Batch  40  Loss  2.8363656997680664\n","Batch  41  Loss  2.8293251991271973\n","Batch  42  Loss  2.974925994873047\n","Epoch: 3 , Loss: tensor(0.5469, device='cuda:0', grad_fn=<AddBackward0>)\n","Batch  1  Loss  2.9269189834594727\n","Batch  2  Loss  2.8501338958740234\n","Batch  3  Loss  2.7364814281463623\n","Batch  4  Loss  2.995068311691284\n","Batch  5  Loss  2.9224750995635986\n","Batch  6  Loss  2.8395493030548096\n","Batch  7  Loss  2.8524811267852783\n","Batch  8  Loss  2.8219738006591797\n","Batch  9  Loss  2.8818604946136475\n","Batch  10  Loss  2.706855058670044\n","Batch  11  Loss  2.8743908405303955\n","Batch  12  Loss  2.8337504863739014\n","Batch  13  Loss  2.782677173614502\n","Batch  14  Loss  2.7099173069000244\n","Batch  15  Loss  2.843865156173706\n","Batch  16  Loss  2.831179141998291\n","Batch  17  Loss  2.822253465652466\n","Batch  18  Loss  2.7782838344573975\n","Batch  19  Loss  2.8164281845092773\n","Batch  20  Loss  2.888174057006836\n","Batch  21  Loss  2.9032349586486816\n","Batch  22  Loss  2.951763868331909\n","Batch  23  Loss  2.8591625690460205\n","Batch  24  Loss  2.842740058898926\n","Batch  25  Loss  2.8904576301574707\n","Batch  26  Loss  2.6650807857513428\n","Batch  27  Loss  2.7723886966705322\n","Batch  28  Loss  2.753133535385132\n","Batch  29  Loss  2.7308671474456787\n","Batch  30  Loss  2.8759686946868896\n","Batch  31  Loss  2.8616318702697754\n","Batch  32  Loss  2.7875709533691406\n","Batch  33  Loss  2.665799617767334\n","Batch  34  Loss  2.752178430557251\n","Batch  35  Loss  2.8535287380218506\n","Batch  36  Loss  2.820826768875122\n","Batch  37  Loss  2.8437418937683105\n","Batch  38  Loss  2.827573537826538\n","Batch  39  Loss  2.8389344215393066\n","Batch  40  Loss  2.8420543670654297\n","Batch  41  Loss  2.827845573425293\n","Batch  42  Loss  2.6429684162139893\n","Epoch: 4 , Loss: tensor(0.5353, device='cuda:0', grad_fn=<AddBackward0>)\n","Batch  1  Loss  2.7955517768859863\n","Batch  2  Loss  2.8024892807006836\n","Batch  3  Loss  2.8761582374572754\n","Batch  4  Loss  2.915940523147583\n","Batch  5  Loss  2.930051803588867\n","Batch  6  Loss  2.9444546699523926\n","Batch  7  Loss  2.854299545288086\n","Batch  8  Loss  2.7960262298583984\n","Batch  9  Loss  2.923988103866577\n","Batch  10  Loss  2.9091010093688965\n","Batch  11  Loss  2.800065040588379\n","Batch  12  Loss  2.684849500656128\n","Batch  13  Loss  2.852806329727173\n","Batch  14  Loss  2.9206318855285645\n","Batch  15  Loss  2.838813066482544\n","Batch  16  Loss  2.931485891342163\n","Batch  17  Loss  2.887612819671631\n","Batch  18  Loss  2.816653251647949\n","Batch  19  Loss  2.829244375228882\n","Batch  20  Loss  2.850876569747925\n","Batch  21  Loss  2.8095200061798096\n","Batch  22  Loss  2.8380298614501953\n","Batch  23  Loss  2.793361186981201\n","Batch  24  Loss  2.830533266067505\n","Batch  25  Loss  2.7694945335388184\n","Batch  26  Loss  2.872297763824463\n","Batch  27  Loss  2.7993738651275635\n","Batch  28  Loss  2.856830596923828\n","Batch  29  Loss  2.829549551010132\n","Batch  30  Loss  2.7613461017608643\n","Batch  31  Loss  2.762814998626709\n","Batch  32  Loss  2.7956435680389404\n","Batch  33  Loss  2.7880802154541016\n","Batch  34  Loss  2.919496536254883\n","Batch  35  Loss  2.7692012786865234\n","Batch  36  Loss  2.798292875289917\n","Batch  37  Loss  2.686218500137329\n","Batch  38  Loss  2.8625059127807617\n","Batch  39  Loss  2.7677314281463623\n","Batch  40  Loss  2.7803046703338623\n","Batch  41  Loss  2.8099257946014404\n","Batch  42  Loss  2.865623950958252\n","Epoch: 5 , Loss: tensor(0.5429, device='cuda:0', grad_fn=<AddBackward0>)\n","Batch  1  Loss  2.8122024536132812\n","Batch  2  Loss  2.7268035411834717\n","Batch  3  Loss  2.8144023418426514\n","Batch  4  Loss  2.7779154777526855\n","Batch  5  Loss  2.967585802078247\n","Batch  6  Loss  2.8013930320739746\n","Batch  7  Loss  2.8687121868133545\n","Batch  8  Loss  2.974702835083008\n","Batch  9  Loss  2.855724334716797\n","Batch  10  Loss  2.8237433433532715\n","Batch  11  Loss  2.8077609539031982\n","Batch  12  Loss  2.890148639678955\n","Batch  13  Loss  2.918471574783325\n","Batch  14  Loss  2.588942289352417\n","Batch  15  Loss  2.8883445262908936\n","Batch  16  Loss  2.763498544692993\n","Batch  17  Loss  2.82452654838562\n","Batch  18  Loss  2.846029281616211\n","Batch  19  Loss  2.8472766876220703\n","Batch  20  Loss  2.9023263454437256\n","Batch  21  Loss  2.917160749435425\n","Batch  22  Loss  2.7234644889831543\n","Batch  23  Loss  2.8411448001861572\n","Batch  24  Loss  2.9001662731170654\n","Batch  25  Loss  2.874284267425537\n","Batch  26  Loss  2.708709716796875\n","Batch  27  Loss  2.8352153301239014\n","Batch  28  Loss  2.7778241634368896\n","Batch  29  Loss  2.9162068367004395\n","Batch  30  Loss  2.850792407989502\n","Batch  31  Loss  2.8139777183532715\n","Batch  32  Loss  2.83095645904541\n","Batch  33  Loss  2.856231927871704\n","Batch  34  Loss  2.837317705154419\n","Batch  35  Loss  2.719053268432617\n","Batch  36  Loss  2.803879976272583\n","Batch  37  Loss  2.8646152019500732\n","Batch  38  Loss  2.7511250972747803\n","Batch  39  Loss  2.726560115814209\n","Batch  40  Loss  2.860531806945801\n","Batch  41  Loss  2.7992312908172607\n","Batch  42  Loss  2.6436116695404053\n","Epoch: 6 , Loss: tensor(0.5354, device='cuda:0', grad_fn=<AddBackward0>)\n","Batch  1  Loss  2.7977283000946045\n","Batch  2  Loss  2.712693214416504\n","Batch  3  Loss  2.7778286933898926\n","Batch  4  Loss  2.806387186050415\n","Batch  5  Loss  2.840026378631592\n","Batch  6  Loss  2.874889373779297\n","Batch  7  Loss  2.7912797927856445\n","Batch  8  Loss  2.9914915561676025\n","Batch  9  Loss  2.8587818145751953\n","Batch  10  Loss  2.8141586780548096\n","Batch  11  Loss  2.7396035194396973\n","Batch  12  Loss  2.867973566055298\n","Batch  13  Loss  2.9418559074401855\n","Batch  14  Loss  2.773052453994751\n","Batch  15  Loss  2.872847080230713\n","Batch  16  Loss  2.7932658195495605\n","Batch  17  Loss  3.004171133041382\n","Batch  18  Loss  2.883699417114258\n","Batch  19  Loss  2.837378740310669\n","Batch  20  Loss  2.810356616973877\n","Batch  21  Loss  2.8790395259857178\n","Batch  22  Loss  2.6658875942230225\n","Batch  23  Loss  2.788424253463745\n","Batch  24  Loss  2.728916883468628\n","Batch  25  Loss  2.699014902114868\n","Batch  26  Loss  2.7945733070373535\n","Batch  27  Loss  2.852389335632324\n","Batch  28  Loss  2.8277535438537598\n","Batch  29  Loss  2.779336929321289\n","Batch  30  Loss  2.8853440284729004\n","Batch  31  Loss  2.7682130336761475\n","Batch  32  Loss  2.799863576889038\n","Batch  33  Loss  2.7710466384887695\n","Batch  34  Loss  2.777923583984375\n","Batch  35  Loss  2.7440881729125977\n","Batch  36  Loss  2.853405475616455\n","Batch  37  Loss  2.7693140506744385\n","Batch  38  Loss  2.852814197540283\n","Batch  39  Loss  2.731252431869507\n","Batch  40  Loss  2.8604934215545654\n","Batch  41  Loss  2.849635362625122\n","Batch  42  Loss  2.8295021057128906\n","Epoch: 7 , Loss: tensor(0.5395, device='cuda:0', grad_fn=<AddBackward0>)\n","Batch  1  Loss  2.8049323558807373\n","Batch  2  Loss  2.823082685470581\n","Batch  3  Loss  2.8229222297668457\n","Batch  4  Loss  2.7863104343414307\n","Batch  5  Loss  2.8273351192474365\n","Batch  6  Loss  3.043830394744873\n","Batch  7  Loss  2.8306703567504883\n","Batch  8  Loss  2.9879512786865234\n","Batch  9  Loss  2.745824098587036\n","Batch  10  Loss  2.846226215362549\n","Batch  11  Loss  2.9416329860687256\n","Batch  12  Loss  2.797095775604248\n","Batch  13  Loss  2.7651572227478027\n","Batch  14  Loss  2.871326208114624\n","Batch  15  Loss  2.6890714168548584\n","Batch  16  Loss  2.771681547164917\n","Batch  17  Loss  2.8534164428710938\n","Batch  18  Loss  2.8189468383789062\n","Batch  19  Loss  2.9496843814849854\n","Batch  20  Loss  2.800297975540161\n","Batch  21  Loss  2.8447256088256836\n","Batch  22  Loss  2.8341593742370605\n","Batch  23  Loss  2.766606092453003\n","Batch  24  Loss  2.757871389389038\n","Batch  25  Loss  2.7050960063934326\n","Batch  26  Loss  2.833961248397827\n","Batch  27  Loss  2.7567334175109863\n","Batch  28  Loss  2.8244121074676514\n","Batch  29  Loss  2.7362911701202393\n","Batch  30  Loss  2.710160493850708\n","Batch  31  Loss  2.701704263687134\n","Batch  32  Loss  2.945146083831787\n","Batch  33  Loss  2.8065478801727295\n","Batch  34  Loss  2.7836685180664062\n","Batch  35  Loss  2.74904203414917\n","Batch  36  Loss  2.9203338623046875\n","Batch  37  Loss  2.8396098613739014\n","Batch  38  Loss  2.740950584411621\n","Batch  39  Loss  2.7079477310180664\n","Batch  40  Loss  2.7512829303741455\n","Batch  41  Loss  2.6855251789093018\n","Batch  42  Loss  2.738424062728882\n","Epoch: 8 , Loss: tensor(0.5355, device='cuda:0', grad_fn=<AddBackward0>)\n","Batch  1  Loss  2.784682273864746\n","Batch  2  Loss  2.851029872894287\n","Batch  3  Loss  2.7923481464385986\n","Batch  4  Loss  2.775681495666504\n","Batch  5  Loss  2.851900339126587\n","Batch  6  Loss  2.8604588508605957\n","Batch  7  Loss  2.7215981483459473\n","Batch  8  Loss  2.8124587535858154\n","Batch  9  Loss  2.862659215927124\n","Batch  10  Loss  2.9171948432922363\n","Batch  11  Loss  2.7798779010772705\n","Batch  12  Loss  2.9040236473083496\n","Batch  13  Loss  2.84295392036438\n","Batch  14  Loss  2.7202415466308594\n","Batch  15  Loss  2.640869379043579\n","Batch  16  Loss  2.7634081840515137\n","Batch  17  Loss  2.847752571105957\n","Batch  18  Loss  2.8201522827148438\n","Batch  19  Loss  2.772376537322998\n","Batch  20  Loss  2.8073198795318604\n","Batch  21  Loss  2.7699265480041504\n","Batch  22  Loss  2.8019087314605713\n","Batch  23  Loss  2.8014461994171143\n","Batch  24  Loss  2.754016637802124\n","Batch  25  Loss  2.7186501026153564\n","Batch  26  Loss  2.7467708587646484\n","Batch  27  Loss  2.7807939052581787\n","Batch  28  Loss  2.9638993740081787\n","Batch  29  Loss  2.765368700027466\n","Batch  30  Loss  2.8381590843200684\n","Batch  31  Loss  2.7936971187591553\n","Batch  32  Loss  2.8424859046936035\n","Batch  33  Loss  2.670410633087158\n","Batch  34  Loss  2.739814519882202\n","Batch  35  Loss  2.8565514087677\n","Batch  36  Loss  2.7974085807800293\n","Batch  37  Loss  2.8617866039276123\n","Batch  38  Loss  2.706308603286743\n","Batch  39  Loss  2.8458900451660156\n","Batch  40  Loss  2.8441712856292725\n","Batch  41  Loss  2.8092236518859863\n","Batch  42  Loss  2.6190671920776367\n","Epoch: 9 , Loss: tensor(0.5304, device='cuda:0', grad_fn=<AddBackward0>)\n"]}],"source":["m.train(bloader,classifier, 10, loss_type = \"cell_type\",encoder = auto_transformer)"]},{"cell_type":"code","execution_count":28,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":26628,"status":"ok","timestamp":1733712324179,"user":{"displayName":"Johan Lindqvist","userId":"10341636458737247069"},"user_tz":480},"id":"gDY2v3E_fPtP","outputId":"6d30bc3f-70eb-4657-8d37-56d0460180b5"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor(-2.3294, device='cuda:0')\n"]}],"source":["with torch.no_grad():\n","    acc = 0\n","    val_acc = 0\n","    for batch in bloader:\n","        #f = batch.obsm['X_scvi'].float().to('cuda')\n","        f = auto_transformer(batch.X.float().to('cuda'))\n","        acc += f[:,:,2].sum()/(59480*256*len(bloader))\n","        #l = torch.tensor(classifier2.encoder.transform(batch.obs['supercluster_term'])).to('cuda')\n","        #batch_acc = classifier.predict_acc(f,l)\n","        #print(batch_acc)\n","        #val_acc += batch_acc/len(cloader)\n","print(acc)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6257,"status":"ok","timestamp":1733161983505,"user":{"displayName":"Johan Lindqvist","userId":"10341636458737247069"},"user_tz":480},"id":"ylsEHXBOYK-V","outputId":"c122dfc0-b6e5-4bcb-9fe3-5a0639d8aa75"},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([256, 59480, 3])\n","tensor(1.4380e+08, device='cuda:0')\n","0.0859375\n","torch.Size([256, 59480, 3])\n","tensor(1.4322e+08, device='cuda:0')\n","0.0390625\n","torch.Size([256, 59480, 3])\n","tensor(1.4397e+08, device='cuda:0')\n","0.06640625\n","torch.Size([256, 59480, 3])\n","tensor(1.4340e+08, device='cuda:0')\n","0.0625\n","torch.Size([256, 59480, 3])\n","tensor(1.4387e+08, device='cuda:0')\n","0.06640625\n","torch.Size([256, 59480, 3])\n","tensor(1.4359e+08, device='cuda:0')\n","0.05859375\n","torch.Size([256, 59480, 3])\n","tensor(1.4356e+08, device='cuda:0')\n","0.078125\n","torch.Size([256, 59480, 3])\n","tensor(1.4337e+08, device='cuda:0')\n","0.078125\n","torch.Size([256, 59480, 3])\n","tensor(1.4405e+08, device='cuda:0')\n","0.046875\n","torch.Size([256, 59480, 3])\n","tensor(1.4424e+08, device='cuda:0')\n","0.09765625\n","torch.Size([256, 59480, 3])\n","tensor(1.4389e+08, device='cuda:0')\n","0.109375\n","torch.Size([256, 59480, 3])\n","tensor(1.4404e+08, device='cuda:0')\n","0.06640625\n","torch.Size([256, 59480, 3])\n","tensor(1.4387e+08, device='cuda:0')\n","0.06640625\n","torch.Size([256, 59480, 3])\n","tensor(1.4396e+08, device='cuda:0')\n","0.05859375\n","torch.Size([256, 59480, 3])\n","tensor(1.4385e+08, device='cuda:0')\n","0.06640625\n","torch.Size([256, 59480, 3])\n","tensor(1.4336e+08, device='cuda:0')\n","0.0625\n","torch.Size([256, 59480, 3])\n","tensor(1.4324e+08, device='cuda:0')\n","0.04296875\n","torch.Size([256, 59480, 3])\n","tensor(1.4373e+08, device='cuda:0')\n","0.05859375\n","torch.Size([256, 59480, 3])\n","tensor(1.4364e+08, device='cuda:0')\n","0.0859375\n","torch.Size([256, 59480, 3])\n","tensor(1.4318e+08, device='cuda:0')\n","0.03515625\n","torch.Size([256, 59480, 3])\n","tensor(1.4389e+08, device='cuda:0')\n","0.05859375\n","torch.Size([256, 59480, 3])\n","tensor(1.4376e+08, device='cuda:0')\n","0.07421875\n","torch.Size([256, 59480, 3])\n","tensor(1.4385e+08, device='cuda:0')\n","0.09375\n","torch.Size([256, 59480, 3])\n","tensor(1.4390e+08, device='cuda:0')\n","0.05078125\n","torch.Size([256, 59480, 3])\n","tensor(1.4324e+08, device='cuda:0')\n","0.08203125\n","torch.Size([256, 59480, 3])\n","tensor(1.4366e+08, device='cuda:0')\n","0.05078125\n","torch.Size([256, 59480, 3])\n","tensor(1.4389e+08, device='cuda:0')\n","0.07421875\n","torch.Size([256, 59480, 3])\n","tensor(1.4355e+08, device='cuda:0')\n","0.05859375\n","torch.Size([256, 59480, 3])\n","tensor(1.4365e+08, device='cuda:0')\n","0.078125\n","torch.Size([256, 59480, 3])\n","tensor(1.4415e+08, device='cuda:0')\n","0.0625\n","torch.Size([256, 59480, 3])\n","tensor(1.4375e+08, device='cuda:0')\n","0.0703125\n","torch.Size([256, 59480, 3])\n","tensor(1.4376e+08, device='cuda:0')\n","0.05859375\n","torch.Size([256, 59480, 3])\n","tensor(1.4390e+08, device='cuda:0')\n","0.03515625\n","torch.Size([256, 59480, 3])\n","tensor(1.4408e+08, device='cuda:0')\n","0.10546875\n","torch.Size([256, 59480, 3])\n","tensor(1.4402e+08, device='cuda:0')\n","0.06640625\n","torch.Size([256, 59480, 3])\n","tensor(1.4372e+08, device='cuda:0')\n","0.06640625\n","torch.Size([256, 59480, 3])\n","tensor(1.4419e+08, device='cuda:0')\n","0.046875\n","torch.Size([256, 59480, 3])\n","tensor(1.4334e+08, device='cuda:0')\n","0.0859375\n","torch.Size([256, 59480, 3])\n","tensor(1.4356e+08, device='cuda:0')\n","0.046875\n","torch.Size([256, 59480, 3])\n","tensor(1.4350e+08, device='cuda:0')\n","0.08984375\n","torch.Size([256, 59480, 3])\n","tensor(1.4372e+08, device='cuda:0')\n","0.09375\n","torch.Size([32, 59480, 3])\n","tensor(18032126., device='cuda:0')\n","0.09375\n"]}],"source":["with torch.no_grad():\n","    acc = 0\n","    val_acc = 0\n","    for batch in cloader:\n","        f = auto_transformer.encode(batch.X.float().to('cuda'))\n","        out = auto_transformer.decode(f)\n","        print(out.shape)\n","        print(out[:,:,0].sum())\n","        l = torch.tensor(classifier.encoder.transform(batch.obs['supercluster_term']))\n","        batch_acc = classifier.predict_acc(f,l)\n","        print(batch_acc)\n","        val_acc += batch_acc/len(cloader)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":221,"status":"ok","timestamp":1732842608197,"user":{"displayName":"Johan Lindqvist","userId":"10341636458737247069"},"user_tz":480},"id":"V1YfvAjuYneX","outputId":"6c1f6705-f151-41f6-9ec3-5b0ceda57c40"},"outputs":[{"data":{"text/plain":["0.3874627976190476"]},"execution_count":47,"metadata":{},"output_type":"execute_result"}],"source":["val_acc"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6ej79RUMdtUb"},"outputs":[],"source":["# MSE Autotransformer with basic classifier network, test_acc for cell_type classificiation 0.22247023809523803\n","# ZINB Autotransformer with basic classifier network, test_acc for cell_type classificiation 0.3874627976190476"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyOkwoT2qGv3J2aqRpQx+3C0","gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"2d2568c263224298ae8bf720c62a64ac":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2dcbe4b801e941c8b6d9a66f645c90cc","placeholder":"​","style":"IPY_MODEL_38319ba0d9ee467e8f4bfa86d444f427","value":" 10/10 [01:14&lt;00:00,  7.21s/it, v_num=1, train_loss_step=1.17e+4, train_loss_epoch=1.42e+4]"}},"2dcbe4b801e941c8b6d9a66f645c90cc":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"349313de85b149a6beb69c6daa005920":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"38319ba0d9ee467e8f4bfa86d444f427":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"57d354dc7c0b4e57b245c90d7ae14097":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7d9ea1446ea34cac9654dc7d5c5e0e14":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"912c27a63f4e419da1510f60b09e8106":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c966977809b145398a53c651dd5b3cf0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_349313de85b149a6beb69c6daa005920","placeholder":"​","style":"IPY_MODEL_d63a9230eceb4a61ac8c21f4413960aa","value":"Epoch 10/10: 100%"}},"d63a9230eceb4a61ac8c21f4413960aa":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e4240bdb6c52412e9c9dfa627f3b01ad":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c966977809b145398a53c651dd5b3cf0","IPY_MODEL_f0e3c76cf540491d943c6dde5dceded4","IPY_MODEL_2d2568c263224298ae8bf720c62a64ac"],"layout":"IPY_MODEL_57d354dc7c0b4e57b245c90d7ae14097"}},"f0e3c76cf540491d943c6dde5dceded4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7d9ea1446ea34cac9654dc7d5c5e0e14","max":10,"min":0,"orientation":"horizontal","style":"IPY_MODEL_912c27a63f4e419da1510f60b09e8106","value":10}}}}},"nbformat":4,"nbformat_minor":0}
